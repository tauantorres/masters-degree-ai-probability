{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d07e049",
   "metadata": {},
   "source": [
    "# 4. `Conditional Probability`\n",
    "\n",
    "1. Incomplete information and sub-sigma-algebras. \n",
    "2. Bayes’ formula as a ratio of measures. \n",
    "3. Prior probability and its update. \n",
    "4. Definition of independence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453e7b37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b6c8b6f",
   "metadata": {},
   "source": [
    "# `4. Conditional Probability`\n",
    "\n",
    "## `1. Incomplete information and sub-sigma-algebras`\n",
    "\n",
    "### Motivation: “updating after we learn something”\n",
    "When we learn that some event **B happened**, we **restrict** attention to the outcomes inside **B**, and then **renormalise** so that total probability becomes 1 again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3759dc5f",
   "metadata": {},
   "source": [
    "### **`Def. 2.2.1`: Conditional probability**\n",
    "If $A$ and $B$ are events with $P(B)>0$, then the conditional probability of $A$ given $B$ is\n",
    "$$\n",
    "P(A\\mid B)=\\frac{P(A\\cap B)}{P(B)}.\n",
    "$$\n",
    "The slides call:\n",
    "- $P(A)$ the **prior probability** of $A$,\n",
    "- $P(A\\mid B)$ the **posterior probability** of $A$ (updated after observing $B$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db35c81",
   "metadata": {},
   "source": [
    "\n",
    "### Intuition 2.2.3 (Pebble world): “restrict + renormalise”\n",
    "1) $B$ occurred, so only take pebbles from $B$  \n",
    "2) Renormalise, so that total mass is still $1$  \n",
    "So you get a new probability scale “inside $B$”."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605aeab8",
   "metadata": {},
   "source": [
    "\n",
    "### Incomplete information as a *partition* (what the slides use)\n",
    "Often our information is not “exact outcome”, but “which *region* we are in”.\n",
    "That is represented by a partition $A_1,\\dots,A_n$ of the sample space:\n",
    "- $A_i$ are disjoint\n",
    "- $\\bigcup_{i=1}^n A_i = S$ (the sample space)\n",
    "\n",
    "This is the setup for the Law of Total Probability.\n",
    "\n",
    "> Note: your topic list mentions **sub-$\\sigma$-algebras**. In these slides, the “partial information” is handled mainly via conditioning on an event $B$ or on a partition $\\{A_i\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5e51a5",
   "metadata": {},
   "source": [
    "## `2. Bayes’ formula as a ratio of measures`\n",
    "\n",
    "### The key identity behind Bayes\n",
    "**`Thm. 2.3.1`: Probability of intersection**\n",
    "For events $A,B$ with positive probabilities:\n",
    "$$\n",
    "P(A\\cap B)=P(B)\\,P(A\\mid B)=P(A)\\,P(B\\mid A).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446ad1b3",
   "metadata": {},
   "source": [
    "### **`Thm. 2.3.3`: Bayes’ rule**\n",
    "From the intersection identity:\n",
    "$$\n",
    "P(A\\mid B)=\\frac{P(B\\mid A)\\,P(A)}{P(B)}.\n",
    "$$\n",
    "Interpretation: posterior = (likelihood × prior) / evidence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331f11ef",
   "metadata": {},
   "source": [
    "\n",
    "### **`Def. 2.3.4`: Odds**\n",
    "For an event $A$, the **odds** are\n",
    "$$\n",
    "\\text{odds}(A)=\\frac{P(A)}{P(A^c)}.\n",
    "$$\n",
    "Example stated: if $P(A)=2/3$, then odds are $2:1$. Conversely:\n",
    "$$\n",
    "P(A)=\\frac{\\text{odds}(A)}{1+\\text{odds}(A)}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21221a2d",
   "metadata": {},
   "source": [
    "### **`Thm. 2.3.5`: Odds form of Bayes’ rule**\n",
    "For events $A,B$ with positive probabilities:\n",
    "$$\n",
    "\\frac{P(A\\mid B)}{P(A^c\\mid B)}\n",
    "=\n",
    "\\frac{P(B\\mid A)}{P(B\\mid A^c)}\n",
    "\\cdot\n",
    "\\frac{P(A)}{P(A^c)}.\n",
    "$$\n",
    "The factor\n",
    "$$\n",
    "\\frac{P(B\\mid A)}{P(B\\mid A^c)}\n",
    "$$\n",
    "is called the **likelihood ratio** in the slides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b8be45",
   "metadata": {},
   "source": [
    "## `3. Prior probability and its update`\n",
    "\n",
    "### Prior vs posterior (language used in the slides)\n",
    "- **Prior**: $P(A)$ (before observing evidence)\n",
    "- **Posterior**: $P(A\\mid B)$ (after observing evidence $B$)\n",
    "\n",
    "This is exactly what Bayes’ rule computes:\n",
    "$$\n",
    "P(A\\mid B)=\\frac{P(B\\mid A)\\,P(A)}{P(B)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d76b4fc",
   "metadata": {},
   "source": [
    "\n",
    "### **`Thm. 2.3.6`: Law of total probability (LOTP)**\n",
    "If $A_1,\\dots,A_n$ is a partition of $S$ and $P(A_i)>0$, then for any event $B$:\n",
    "$$\n",
    "P(B)=\\sum_{i=1}^n P(B\\mid A_i)\\,P(A_i).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31504458",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Example 2.3.9 (Testing for a disease): update can be surprising\n",
    "Given:\n",
    "- prevalence (prior): $P(D)=0.01$\n",
    "- test sensitivity: $P(T\\mid D)=0.95$\n",
    "- true negative: $P(T^c\\mid D^c)=0.95$ so $P(T\\mid D^c)=0.05$\n",
    "\n",
    "Then:\n",
    "$$\n",
    "P(D\\mid T)\n",
    "=\n",
    "\\frac{P(T\\mid D)\\,P(D)}\n",
    "{P(T\\mid D)\\,P(D)+P(T\\mid D^c)\\,P(D^c)}\n",
    "\\approx 0.16.\n",
    "$$\n",
    "So even a “95% accurate” test can lead to a posterior of only ~16% when the prior is very small.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef40511",
   "metadata": {},
   "source": [
    "### Bayes with extra conditioning (updating *given* some background info)\n",
    "**`Thm. 2.4.2`: Bayes’ rule with extra conditioning**\n",
    "Provided $P(A\\cap E)>0$ and $P(B\\cap E)>0$:\n",
    "$$\n",
    "P(A\\mid B,E)=\\frac{P(B\\mid A,E)\\,P(A\\mid E)}{P(B\\mid E)}.\n",
    "$$\n",
    "\n",
    "(And the slides also give LOTP with extra conditioning using the same idea.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d083c3fd",
   "metadata": {},
   "source": [
    "## `4. Definition of independence`\n",
    "\n",
    "### **`Def. 2.5.1`: Independence of two events**\n",
    "Events $A$ and $B$ are **independent** if:\n",
    "$$\n",
    "P(A\\cap B)=P(A)\\,P(B).\n",
    "$$\n",
    "If $P(A)>0$ and $P(B)>0$, this is equivalent to:\n",
    "$$\n",
    "P(A\\mid B)=P(A)\n",
    "\\quad\\text{and also to}\\quad\n",
    "P(B\\mid A)=P(B).\n",
    "$$\n",
    "Independence is symmetric: if $A$ is independent of $B$, then $B$ is independent of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0ac3c1",
   "metadata": {},
   "source": [
    "### Important note (slides emphasize this)\n",
    "Independence $\\neq$ disjointness.  \n",
    "If $A$ and $B$ are disjoint, then $P(A\\cap B)=0$, so they can be independent only if\n",
    "$$\n",
    "P(A)=0 \\quad \\text{or} \\quad P(B)=0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1fc3d9",
   "metadata": {},
   "source": [
    "### Proposition 2.5.3 (closure under complements)\n",
    "If $A$ and $B$ are independent, then the slides state that these pairs are also independent:\n",
    "- $A$ and $B^c$\n",
    "- $A^c$ and $B$\n",
    "- $A^c$ and $B^c$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b221d3",
   "metadata": {},
   "source": [
    "### **`Def. 2.5.4`: Independence of 3 events**\n",
    "Events $A,B,C$ are independent if all of these hold:\n",
    "$$\n",
    "P(A\\cap B)=P(A)P(B),\\quad\n",
    "P(A\\cap C)=P(A)P(C),\\quad\n",
    "P(B\\cap C)=P(B)P(C),\n",
    "$$\n",
    "and\n",
    "$$\n",
    "P(A\\cap B\\cap C)=P(A)P(B)P(C).\n",
    "$$\n",
    "If only the first three hold, the slides call that **pairwise independence**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2918d22f",
   "metadata": {},
   "source": [
    "### Example 2.5.5 (pairwise independent but not independent)\n",
    "Two coin tosses:\n",
    "- $A$ = “1st is H”\n",
    "- $B$ = “2nd is H”\n",
    "- $C$ = “both tosses have same result”\n",
    "\n",
    "Slides state they are pairwise independent, but:\n",
    "$$\n",
    "P(A\\cap B\\cap C)=\\frac14\n",
    "\\quad \\text{while} \\quad\n",
    "P(A)P(B)P(C)=\\frac18,\n",
    "$$\n",
    "so they are **not** independent as a triple.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daa5ea3",
   "metadata": {},
   "source": [
    "\n",
    "### **`Def. 2.5.7`: Conditional independence**\n",
    "Events $A$ and $B$ are conditionally independent given $E$ if:\n",
    "$$\n",
    "P(A\\cap B\\mid E)=P(A\\mid E)\\,P(B\\mid E).\n",
    "$$\n",
    "The slides warn:\n",
    "- conditional independence does **not** imply independence\n",
    "- independence does **not** imply conditional independence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff15fcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "342cda2e",
   "metadata": {},
   "source": [
    "# `4. Conditional Probability`  *(updated with the new slides)*\n",
    "\n",
    "## `1. Incomplete information and sub-sigma-algebras`\n",
    "\n",
    "### Motivation: “conditioning on what you know”\n",
    "So far we usually conditioned on events (like $P(A\\mid B)$). But in real situations we often have **partial information**: we can distinguish some events, but not others.\n",
    "\n",
    "The slides formalize “what you know” as a **sub-$\\sigma$-algebra**\n",
    "$$\n",
    "\\mathcal G \\subseteq \\mathcal F,\n",
    "$$\n",
    "which contains exactly the events we can tell apart with our limited information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9821b367",
   "metadata": {},
   "source": [
    "### How do sub-$\\sigma$-algebras come from observations?\n",
    "- If we observe the actual value of a random variable $Y:\\Omega\\to\\mathcal Y$, then the natural information is\n",
    "$$\n",
    "\\mathcal G=\\sigma(Y),\n",
    "$$\n",
    "the $\\sigma$-algebra generated by $Y$.\n",
    "\n",
    "- If we only observe whether an event $E$ happened (coarse information), then\n",
    "$$\n",
    "\\mathcal G=\\{\\varnothing, E, E^c, \\Omega\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308fc0ea",
   "metadata": {},
   "source": [
    "### What should “conditional probability given $\\mathcal G$” look like?\n",
    "Fix an event $A\\in\\mathcal F$. The slides state that “the updated probability” should be:\n",
    "\n",
    "1) **Random**: it depends on what was observed, so it must be a random variable.\n",
    "2) **Consistent**: averaging over all possible observations should recover the original probability (marginalisation).\n",
    "\n",
    "So we want a **$\\mathcal G$-measurable random variable** $Z(\\omega)$ that represents “$P(A \\mid \\text{what we know})”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c68c7a",
   "metadata": {},
   "source": [
    "## `2. Bayes’ formula as a ratio of measures`\n",
    "\n",
    "### **`Def. 2.2.1`: Conditional probability (event version)**\n",
    "If $A,B\\in\\mathcal F$ and $P(B)>0$:\n",
    "$$\n",
    "P(A\\mid B)=\\frac{P(A\\cap B)}{P(B)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32040c7f",
   "metadata": {},
   "source": [
    "### **`Thm. 2.3.1`: Probability of intersection**\n",
    "For events $A,B$ with positive probabilities:\n",
    "$$\n",
    "P(A\\cap B)=P(B)\\,P(A\\mid B)=P(A)\\,P(B\\mid A).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b14668",
   "metadata": {},
   "source": [
    "### **`Thm. 2.3.3`: Bayes’ rule**\n",
    "$$\n",
    "P(A\\mid B)=\\frac{P(B\\mid A)\\,P(A)}{P(B)}.\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b65d3c",
   "metadata": {},
   "source": [
    "### **`Def. 2.3.4`: Odds**\n",
    "$$\n",
    "\\text{odds}(A)=\\frac{P(A)}{P(A^c)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64034d3a",
   "metadata": {},
   "source": [
    "### **`Thm. 2.3.5`: Odds form of Bayes’ rule**\n",
    "$$\n",
    "\\frac{P(A\\mid B)}{P(A^c\\mid B)}\n",
    "=\n",
    "\\frac{P(B\\mid A)}{P(B\\mid A^c)}\n",
    "\\cdot\n",
    "\\frac{P(A)}{P(A^c)}.\n",
    "$$\n",
    "The factor\n",
    "$$\n",
    "\\frac{P(B\\mid A)}{P(B\\mid A^c)}\n",
    "$$\n",
    "is the **likelihood ratio**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224fbcc9",
   "metadata": {},
   "source": [
    "## `3. Prior probability and its update`\n",
    "\n",
    "### Prior vs posterior\n",
    "- **Prior**: $P(A)$ (before observing evidence)\n",
    "- **Posterior**: $P(A\\mid B)$ (after observing evidence $B$)\n",
    "\n",
    "Bayes performs the update:\n",
    "$$\n",
    "P(A\\mid B)=\\frac{P(B\\mid A)\\,P(A)}{P(B)}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd41ada",
   "metadata": {},
   "source": [
    "### **`Thm. 2.3.6`: Law of total probability (LOTP)**\n",
    "If $A_1,\\dots,A_n$ is a partition of $S$ with $P(A_i)>0$, then for any $B$:\n",
    "$$\n",
    "P(B)=\\sum_{i=1}^n P(B\\mid A_i)\\,P(A_i).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1f68b8",
   "metadata": {},
   "source": [
    "### Example 2.3.9 (disease testing): small prior ⇒ surprising posterior\n",
    "The slides show a case where even a “95% accurate” test can yield\n",
    "a posterior around $0.16$ when the disease prevalence prior is $0.01$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9983c418",
   "metadata": {},
   "source": [
    "### **`Thm. 2.4.2`: Bayes’ rule with extra conditioning**\n",
    "If $P(A\\cap E)>0$ and $P(B\\cap E)>0$:\n",
    "$$\n",
    "P(A\\mid B,E)=\\frac{P(B\\mid A,E)\\,P(A\\mid E)}{P(B\\mid E)}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548cf2c4",
   "metadata": {},
   "source": [
    "## `1 (extended). Conditional probability given partial information (sub-$\\sigma$-algebra)`\n",
    "\n",
    "### What properties should $P(A\\mid \\mathcal G)$ satisfy?\n",
    "For fixed $A\\in\\mathcal F$, the slides define conditional probability given $\\mathcal G$\n",
    "as a **$\\mathcal G$-measurable random variable** $Z$ such that:\n",
    "$$\n",
    "\\int_G Z(\\omega)\\,dP(\\omega)=P(A\\cap G)\n",
    "\\quad \\forall\\,G\\in\\mathcal G.\n",
    "$$\n",
    "- $Z$ must be $\\mathcal G$-measurable (computable from the available information),\n",
    "- the integral identity is the **consistency condition**.\n",
    "\n",
    "This object is denoted:\n",
    "$$\n",
    "P(A\\mid \\mathcal G):=Z.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431b35d0",
   "metadata": {},
   "source": [
    "### Measurability matters (interpretation)\n",
    "\n",
    "A $\\mathcal G$-measurable random variable is **constant on the atoms of $\\mathcal G$**.\n",
    "So you can’t condition on events “finer” than your information allows.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3019293",
   "metadata": {},
   "source": [
    "### RN construction (why Radon–Nikodym appears)\n",
    "Fix $A\\in\\mathcal F$ and define a finite measure on $(\\Omega,\\mathcal G)$ by:\n",
    "$$\n",
    "\\nu(G):=P(A\\cap G),\\quad G\\in\\mathcal G.\n",
    "$$\n",
    "Then:\n",
    "- $\\nu$ is finite,\n",
    "- and $\\nu \\ll P|_{\\mathcal G}$ (restriction of $P$ to $\\mathcal G$).  \n",
    "\n",
    "By the RN theorem, there exists a $\\mathcal G$-measurable $Z$ such that:\n",
    "$$\n",
    "\\nu(G)=\\int_G Z\\,dP \\quad \\forall\\,G\\in\\mathcal G.\n",
    "$$\n",
    "We then define:\n",
    "$$\n",
    "P(A\\mid \\mathcal G):=Z\n",
    "\\quad\\text{and}\\quad\n",
    "P(A\\mid \\mathcal G)=\\frac{d\\nu}{dP|_{\\mathcal G}}.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ca5dbb",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Indicator / expectation form (slides)\n",
    "The slides also write:\n",
    "$$\n",
    "P(A\\mid \\mathcal G)=\\mathbb E[\\mathbf 1_A\\mid \\mathcal G],\n",
    "$$\n",
    "meaning: conditional probability is a special case of conditional expectation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78ffaea",
   "metadata": {},
   "source": [
    "### Why the RN approach? (slides’ bullet points)\n",
    "- Ensures $\\mathcal G$-measurability + consistency, so averaging recovers unconditional joint probabilities.\n",
    "- Generalises finite-partition conditioning: if $\\mathcal G$ is generated by a finite partition $\\{G_i\\}$, then $P(A\\mid \\mathcal G)$ is constant on each atom $G_i$ and matches the usual discrete conditional probabilities.\n",
    "- Gives a **single object** $P(A\\mid \\mathcal G)$ defined on $\\Omega$, not just a family of numbers $P(A\\mid G_i)$ (important in continuous cases).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8cb58b",
   "metadata": {},
   "source": [
    "\n",
    "## `4. Definition of independence`\n",
    "\n",
    "### **`Def. 2.5.1`: Independence of two events**\n",
    "Events $A$ and $B$ are independent if:\n",
    "$$\n",
    "P(A\\cap B)=P(A)\\,P(B).\n",
    "$$\n",
    "If $P(A)>0$ and $P(B)>0$, equivalently:\n",
    "$$\n",
    "P(A\\mid B)=P(A)\n",
    "\\quad\\text{and}\\quad\n",
    "P(B\\mid A)=P(B).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc47fae9",
   "metadata": {},
   "source": [
    "### Independence ≠ disjointness\n",
    "If $A$ and $B$ are disjoint, then $P(A\\cap B)=0$, so they can be independent only if\n",
    "$$\n",
    "P(A)=0 \\ \\text{or}\\ P(B)=0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba09c23",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Proposition 2.5.3 (closure under complements)\n",
    "If $A$ and $B$ are independent, then:\n",
    "- $A$ and $B^c$ are independent\n",
    "- $A^c$ and $B$ are independent\n",
    "- $A^c$ and $B^c$ are independent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e1a1bc",
   "metadata": {},
   "source": [
    "### **`Def. 2.5.4`: Independence of 3 events**\n",
    "$A,B,C$ are independent if:\n",
    "$$\n",
    "P(A\\cap B)=P(A)P(B),\\;\n",
    "P(A\\cap C)=P(A)P(C),\\;\n",
    "P(B\\cap C)=P(B)P(C),\n",
    "$$\n",
    "and\n",
    "$$\n",
    "P(A\\cap B\\cap C)=P(A)P(B)P(C).\n",
    "$$\n",
    "If only the pairwise equalities hold, that is **pairwise independence**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e543c72",
   "metadata": {},
   "source": [
    "\n",
    "### Example 2.5.5 (pairwise independent but not independent)\n",
    "Two coin tosses:\n",
    "- $A$ = “1st is H”\n",
    "- $B$ = “2nd is H”\n",
    "- $C$ = “both tosses have same result”\n",
    "\n",
    "Slides show:\n",
    "$$\n",
    "P(A\\cap B\\cap C)=\\frac14\n",
    "\\neq\n",
    "P(A)P(B)P(C)=\\frac18.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcfb372",
   "metadata": {},
   "source": [
    "### **`Def. 2.5.7`: Conditional independence**\n",
    "$A$ and $B$ are conditionally independent given $E$ if:\n",
    "$$\n",
    "P(A\\cap B\\mid E)=P(A\\mid E)\\,P(B\\mid E).\n",
    "$$\n",
    "Slides warn: conditional independence and independence do not imply each other in general.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7984c898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42592ddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544725cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
