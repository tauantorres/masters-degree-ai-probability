{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff2bead2",
   "metadata": {},
   "source": [
    "# `3. Random Variables`\n",
    "\n",
    "1. ~~Random variables and random elements: motivation and examples.~~\n",
    "2. ~~Transformations of random variables.~~\n",
    "3. ~~Distribution of a random variable.~~\n",
    "4. ~~Characteristic function and moment-generating function.~~\n",
    "5. ~~Moments and cumulants.~~\n",
    "6. Families of distributions. \n",
    "7. Statistics. \n",
    "8. Entropy. \n",
    "9. Independent random variables, marginals.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c47a06",
   "metadata": {},
   "source": [
    "### `1. Random variables and random elements: motivation and examples`\n",
    "\n",
    "### Motivation: why do we even need random variables?\n",
    "Dealing directly with a huge outcome space $\\Omega$ is messy and exhausting. Also, we **don’t measure $\\Omega$ directly** — the probability measure $P$ is defined on the **event space** $\\mathcal F$, not on individual outcomes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce6525e",
   "metadata": {},
   "source": [
    "**`Example 01 (repeated fair die)`:**\n",
    "\n",
    "If we roll a fair 6-sided die $n$ times, the outcomes are length-$n$ sequences, so:\n",
    "$$\n",
    "\\Omega = \\{\\omega_1,\\omega_2,\\omega_3,\\omega_4,\\omega_5,\\omega_6\\}^n,\n",
    "\\qquad |\\Omega|=6^n,\n",
    "$$\n",
    "and with the biggest possible $\\sigma$-algebra:\n",
    "$$\n",
    "\\mathcal F = 2^\\Omega\n",
    "$$\n",
    "(which is the power set).\n",
    "\n",
    "Then the number of measurable events explodes:\n",
    "$$\n",
    "|\\mathcal F| = 2^{|\\Omega|} = 2^{6^n}.\n",
    "$$\n",
    "For $n=2$ that is $2^{36}\\approx 68$ billion events — not cool.\n",
    "\n",
    "Even though for a concrete outcome (a concrete sequence) $\\omega$ we have:\n",
    "$$\n",
    "P(\\{\\omega\\}) = \\left(\\frac{1}{6}\\right)^n = \\frac{1}{|\\Omega|},\n",
    "$$\n",
    "working with such gigantic structures becomes a nightmare for actual calculations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f427be5e",
   "metadata": {},
   "source": [
    "### “Adjusting the scope”: focusing on what you need\n",
    "Instead of tracking every tiny detail in $\\Omega$, we often want **coarser questions**, like:\n",
    "1) “Did we roll only even numbers?”  \n",
    "2) “Did we never roll more than 4?”  \n",
    "3) “Was the $(n-42)$-th roll a 4 or a 5?”\n",
    "\n",
    "So we introduce a **helping tool**: a function that extracts only the information we care about.\n",
    "\n",
    "**Even/odd detector example (the notes’ motivating construction).**  \n",
    "Set $n=3$ and define a function $h$ that maps each roll to Even/ Odd:\n",
    "$$\n",
    "h : \\{2,5,4\\} \\mapsto \\{E,O,E\\}.\n",
    "$$\n",
    "For any “detected pattern” $B\\subset\\{E,O\\}^3$, we can look at the **preimage**:\n",
    "$$\n",
    "h^{-1}(B)=\\{\\text{all original sequences in }\\Omega \\text{ that produce pattern }B\\}.\n",
    "$$\n",
    "Then we define the $\\sigma$-algebra generated by this detector:\n",
    "$$\n",
    "\\sigma(h)=\\{h^{-1}(B)\\mid B\\subset\\{E,O\\}^3\\}.\n",
    "$$\n",
    "Now we only care about 8 patterns (since $2^3=8$), and for “all even” we get:\n",
    "$$\n",
    "P(\\{E,E,E\\})=\\frac{1}{8}.\n",
    "$$\n",
    "\n",
    "This is the key idea: **choose the right function / viewpoint → shrink the complexity.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383ab1e0",
   "metadata": {},
   "source": [
    "### Measurability via preimages (why the trick works)\n",
    "The notes recall the “continuity via preimages” idea and reuse it for measurability: we call a function measurable when the preimage of a measurable set stays measurable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56f1564",
   "metadata": {},
   "source": [
    "### **`Def. 2.2`: Random Variable**\n",
    "Given an outcome (sample) space $(\\Omega,\\mathcal F)$ and a measurable space $(Y,\\mathcal Y)$, an $\\mathcal F$-measurable function\n",
    "$$\n",
    "X:(\\Omega,\\mathcal F)\\to (Y,\\mathcal Y)\n",
    "$$\n",
    "is called a **random variable**.\n",
    "\n",
    "Equivalently (what “$\\mathcal F$-measurable” means here):\n",
    "$$\n",
    "\\forall B\\in\\mathcal Y:\\quad X^{-1}(B)\\in\\mathcal F.\n",
    "$$\n",
    "So events of the form “$X\\in B$” are measurable events in the original probability space.\n",
    "\n",
    "**Common shorthand used in the notes.**  \n",
    "Often we restrict to $(\\mathbb R,\\mathcal B(\\mathbb R))$ and write lazily:\n",
    "$$\n",
    "X:\\Omega\\to\\mathbb R,\n",
    "$$\n",
    "while measurability is still the important hidden requirement.\n",
    "\n",
    "> * **Note**: Random variables help us ”organize” the outcome (sample) space by mapping it to some other space, like a real line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e723798c",
   "metadata": {},
   "source": [
    "### **`Def. 2.2.X`: Random elements** (same definition, more general codomain)\n",
    "A “random variable” is just a **random element** whose codomain is usually $\\mathbb R$.\n",
    "The notes emphasize that different codomains change what kind of data we model:\n",
    "\n",
    "- $(\\mathbb R,\\mathcal B(\\mathbb R))$ — one measurement (a single number)\n",
    "- $(\\mathbb R^n,\\mathcal B(\\mathbb R^n))$ — multiple measurements at once (a vector)\n",
    "- $(\\mathbb R^{m\\times n},\\mathcal B(\\mathbb R^{m\\cdot n}))$ — tabular data (rectangles)\n",
    "- $(S^T,\\mathcal C(T,S))$ — function-valued objects with a cylinder $\\sigma$-algebra (“borderline stochastic stuff”)  \n",
    "\n",
    "So: **random variable** = special case; **random element** = same idea for general $Y$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce8d2bd",
   "metadata": {},
   "source": [
    "### “Not so random variables”\n",
    "The notes point out the classic joke: they are neither truly “random” nor “variables”.\n",
    "They are **functions** that map outcomes (which can be anything) into something measurable (often numbers), so we can exploit the structure of $\\mathbb R$ for calculations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e4fb0c",
   "metadata": {},
   "source": [
    "## `Examples`\n",
    "\n",
    "\n",
    "### `Discrete Random Variables`:\n",
    "\n",
    "\n",
    "#### `Example 1: Bernoulli RV`\n",
    "$$\n",
    "X:\\Omega\\to\\{0,1\\}\n",
    "$$\n",
    "with:\n",
    "- $\\Omega=\\{\\text{Success},\\text{Fail}\\}$\n",
    "- $\\mathcal F=\\{\\varnothing,\\{\\text{Success}\\},\\{\\text{Fail}\\},\\Omega\\}$\n",
    "- $X(\\text{Success})=1,\\; X(\\text{Fail})=0$\n",
    "\n",
    "* **Interpretation**: any “hit or miss” / binary classification situation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6736e9",
   "metadata": {},
   "source": [
    "#### `Example 2: Binomial RV (count successes)`:\n",
    "\n",
    "For $n=3$ trials:\n",
    "$$\n",
    "X:\\Omega\\to\\{0,1,\\dots,n\\},\\qquad \\Omega=\\{S,F\\}^3,\\quad \\mathcal F=2^\\Omega,\n",
    "$$\n",
    "and e.g.\n",
    "$$\n",
    "X(SFF)=1,\\quad X(SSF)=2,\\quad X(SFS)=2.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a1e83c",
   "metadata": {},
   "source": [
    "#### `Example 3: Discrete rank RV`:\n",
    "\n",
    "Ranks player $A$ in a 4-player match:\n",
    "$$\n",
    "X:\\Omega\\to\\{1,2,3,4\\},\n",
    "$$\n",
    "where $\\Omega$ is all $4!$ permutations of $\\{A,B,C,D\\}$, and $\\mathcal F=2^\\Omega$.  \n",
    "Examples:\n",
    "$$\n",
    "X(B,A,C,D)=2,\\quad X(B,C,A,D)=3,\\quad X(B,D,A,C)=3.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3eb1e2",
   "metadata": {},
   "source": [
    "### `Continuous Random Variables`:\n",
    "\n",
    "#### `Example 1: Uniform RV`:\n",
    "$$\n",
    "X:\\Omega\\to[0,1],\n",
    "\\qquad \\mathcal F=\\{\\text{Borel subsets of }[0,1]\\},\n",
    "$$\n",
    "with example values like $X(0.3)=0.3$, $X(0.734)=0.734$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2452e5",
   "metadata": {},
   "source": [
    "#### `Example 2: Exponential RV (waiting time)`:\n",
    "\n",
    "$$\n",
    "X:\\Omega\\to[0,\\infty),\n",
    "\\qquad \\mathcal F=\\{\\text{Borel subsets of }[0,\\infty)\\},\n",
    "$$\n",
    "example values like $X(\\omega_1)=4.20$, $X(\\omega_3)=304$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09859e11",
   "metadata": {},
   "source": [
    "#### `Example 3: Normal RV (deviation from mean)`:\n",
    "\n",
    "$$\n",
    "X:\\Omega\\to\\mathbb R,\n",
    "\\qquad \\mathcal F=\\{\\text{Borel subsets of }\\mathbb R\\},\n",
    "$$\n",
    "example values like $X(\\omega_1)=-5.1$, $X(\\omega_2)=0.1$, $X(\\omega_3)=2.4$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd2b4d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "299e4cfd",
   "metadata": {},
   "source": [
    "## `2. Transformations of random variables`\n",
    "\n",
    "### **What is it / why do we care?**\n",
    "\n",
    "* **What is it**: a transformation is when we take an existing random variable $X$ and build a new one by applying a (measurable) function $g$:\n",
    "$$\n",
    "Y = g(X).\n",
    "$$\n",
    "So we are not “creating new randomness” — we are **repackaging the same underlying randomness** into a new quantity.\n",
    "\n",
    "* **Why we care** (typical reasons):\n",
    "  1. **Derived quantities show up naturally** in applications: if $V$ is speed, energy is $E=\\tfrac12 mV^2$; if $P_t$ is a price, return can be $\\log(P_t/P_{t-1})$, etc.\n",
    "  2. Often we know the distribution of $X$, but we actually need the distribution or expectation of $g(X)$.\n",
    "  3. In simulation, we generate a “simple” $X$ (often uniform) and transform it to get more complex distributions.\n",
    "  4. Many expectations can be computed **without** first finding the density of $Y$ (LOTUS), which saves a lot of work.\n",
    "\n",
    "---\n",
    "\n",
    "### **Definition: transformation of a random variable**\n",
    "\n",
    "Given a probability space $(\\Omega,\\mathcal F,P)$, a random variable $X:\\Omega\\to\\mathbb R$, and a measurable function $g:\\mathbb R\\to\\mathbb R$, define\n",
    "$$\n",
    "Y(\\omega) = g(X(\\omega)), \\qquad \\omega\\in\\Omega.\n",
    "$$\n",
    "Then $Y$ is also a random variable on the same probability space.\n",
    "\n",
    "---\n",
    "\n",
    "## `Core idea`: probability moves through the function (pushforward)\n",
    "\n",
    "A distribution tells you “how probability mass sits on the line”.  \n",
    "Applying $g$ **moves that mass**.\n",
    "\n",
    "For any measurable set $B\\subseteq\\mathbb R$:\n",
    "$$\n",
    "P_Y(B)=P(Y\\in B)\n",
    "= P(g(X)\\in B)\n",
    "= P\\big(X\\in g^{-1}(B)\\big)\n",
    "= P_X\\big(g^{-1}(B)\\big).\n",
    "$$\n",
    "\n",
    "**Interpretation**: to know how likely $Y$ is to land in $B$, look at the set of $x$ values that get mapped into $B$ and measure how likely $X$ is to fall there.\n",
    "\n",
    "---\n",
    "\n",
    "## `LOTUS` (Law of the Unconscious Statistician)\n",
    "\n",
    "This is the main shortcut for expectations.\n",
    "\n",
    "Let $Y=g(X)$. For any integrable function $h$:\n",
    "$$\n",
    "\\mathbb E[h(Y)] = \\mathbb E[h(g(X))].\n",
    "$$\n",
    "\n",
    "More explicitly, in terms of the distribution of $X$:\n",
    "$$\n",
    "\\mathbb E[h(Y)] = \\int_{\\mathbb R} h(g(x))\\, dP_X(x).\n",
    "$$\n",
    "\n",
    "If $X$ has a density $f_X$, then:\n",
    "$$\n",
    "\\mathbb E[h(Y)] = \\int_{\\mathbb R} h(g(x))\\, f_X(x)\\, dx.\n",
    "$$\n",
    "\n",
    "**Why this matters**: many problems ask for $\\mathbb E[g(X)]$ (means, variances, risks, losses). LOTUS lets you compute it directly from $X$.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## How to compute the distribution of $Y=g(X)$ in practice\n",
    "\n",
    "There are three common routes.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### `1) CDF method` (works very generally)\n",
    "\n",
    "Start from:\n",
    "$$\n",
    "F_Y(y) = P(Y\\le y) = P(g(X)\\le y).\n",
    "$$\n",
    "\n",
    "* If $g$ is **strictly increasing**, then $g(X)\\le y \\iff X\\le g^{-1}(y)$, so:\n",
    "$$\n",
    "F_Y(y) = F_X(g^{-1}(y)).\n",
    "$$\n",
    "\n",
    "* If $g$ is **strictly decreasing**, then $g(X)\\le y \\iff X\\ge g^{-1}(y)$, so:\n",
    "$$\n",
    "F_Y(y) = P(X\\ge g^{-1}(y)) = 1 - F_X(g^{-1}(y))\n",
    "$$\n",
    "(with the usual “$\\le$ vs $<$” convention depending on continuity).\n",
    "\n",
    "**When to use**: when you know $F_X$ or when $X$ might not have a density.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### `2) Density change-of-variables` (fast when $g$ is one-to-one)\n",
    "\n",
    "Assume:\n",
    "- $X$ has density $f_X$,\n",
    "- $g$ is differentiable and strictly monotone (so $g^{-1}$ exists).\n",
    "\n",
    "Then $Y=g(X)$ has density:\n",
    "$$\n",
    "f_Y(y) = f_X(g^{-1}(y))\\left|\\frac{d}{dy}g^{-1}(y)\\right|.\n",
    "$$\n",
    "\n",
    "**Intuition**: small probability masses must match:\n",
    "$$\n",
    "P(y\\le Y\\le y+dy) \\approx P(x\\le X\\le x+dx),\n",
    "$$\n",
    "and the mapping rescales lengths by:\n",
    "$$\n",
    "dx = \\left|\\frac{d}{dy}g^{-1}(y)\\right|dy.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### `3) Non-injective (many-to-one) transformations` (folding the line)\n",
    "\n",
    "If $g$ is not one-to-one (e.g., $g(x)=x^2$), a single $y$ can have multiple preimages $x_i$ with $g(x_i)=y$.\n",
    "\n",
    "Under standard regularity assumptions (piecewise monotone, differentiable):\n",
    "$$\n",
    "f_Y(y) = \\sum_{x_i:\\, g(x_i)=y}\\frac{f_X(x_i)}{|g'(x_i)|}.\n",
    "$$\n",
    "\n",
    "**Intuition**: if the function “folds” the real line, probability mass from multiple branches lands on the same $y$, so densities add up.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Discrete case (pmf transformation)\n",
    "\n",
    "If $X$ is discrete with pmf $p_X(x)=P(X=x)$, then $Y=g(X)$ is discrete and:\n",
    "$$\n",
    "p_Y(y)=P(Y=y)=\\sum_{x:\\, g(x)=y} P(X=x)=\\sum_{x:\\, g(x)=y} p_X(x).\n",
    "$$\n",
    "\n",
    "**Intuition**: collect all probability from values of $X$ that map to the same $y$.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Intuition summary: “mass gets reshaped”\n",
    "\n",
    "Think of the distribution of $X$ as “mass on the $x$-axis”.\n",
    "\n",
    "* If $g$ **stretches** distances, density **decreases**.\n",
    "* If $g$ **compresses** distances, density **increases**.\n",
    "* If $g$ **folds** the axis (not one-to-one), densities from different branches **add**.\n",
    "\n",
    "That is exactly what the Jacobian factor and the “sum over preimages” formula express.\n",
    "\n",
    "---\n",
    "\n",
    "## `Example A`: expectation via LOTUS (no need for $f_Y$)\n",
    "\n",
    "Let $X\\sim \\mathrm{Unif}[0,1]$, $Y=2X+1$, and let $h(y)=y^2$.\n",
    "Then:\n",
    "$$\n",
    "\\mathbb E[h(Y)] = \\mathbb E[h(2X+1)]\n",
    "= \\int_0^1 (2x+1)^2\\,dx.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## `Example B`: simple discrete transformation\n",
    "\n",
    "Let $X\\sim \\mathrm{Bernoulli}(p)$ and define $Y=1-X$.\n",
    "Then:\n",
    "$$\n",
    "P(Y=1)=P(X=0)=1-p,\\qquad P(Y=0)=P(X=1)=p,\n",
    "$$\n",
    "so:\n",
    "$$\n",
    "Y\\sim \\mathrm{Bernoulli}(1-p),\n",
    "\\qquad\n",
    "\\mathbb E[Y]=1-p.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## What to say in an oral exam (short + correct)\n",
    "\n",
    "> “A transformation is $Y=g(X)$. The distribution of $Y$ is the pushforward of the distribution of $X$: for measurable $B$,\n",
    "> $$\n",
    "> P(Y\\in B)=P(X\\in g^{-1}(B)).\n",
    "> $$\n",
    "> Practically, I can compute $F_Y(y)=P(g(X)\\le y)$ via CDF manipulation, or if $X$ has a density and $g$ is monotone I use the Jacobian:\n",
    "> $$\n",
    "> f_Y(y)=f_X(g^{-1}(y))\\left|\\frac{d}{dy}g^{-1}(y)\\right|.\n",
    "> $$\n",
    "> If $g$ is not one-to-one, I sum over all preimages. For expectations I use LOTUS:\n",
    "> $$\n",
    "> \\mathbb E[h(Y)] = \\int h(g(x))\\,dP_X(x).\n",
    "> $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d44720e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55e2361c",
   "metadata": {},
   "source": [
    "## `3. Distribution of a random variable`\n",
    "\n",
    "### Motivation: we want probabilities of events like `X ∈ B`\n",
    "Given a probability space $(\\Omega,\\mathcal F,P)$ and a random variable\n",
    "$$\n",
    "X:\\Omega \\to \\mathbb R,\n",
    "$$\n",
    "we often care about events of the form:\n",
    "$$\n",
    "\\{\\omega\\in\\Omega: X(\\omega)\\in B\\} = X^{-1}(B),\n",
    "$$\n",
    "where $B$ is a (Borel) set in $\\mathbb R$.\n",
    "This event is measurable (belongs to $\\mathcal F$), so it has a probability.\n",
    "\n",
    "> * **`Def.`: Borel Set**: A Borel set is any set in the Borel $\\sigma$-algebra $\\mathcal B(\\mathbb R)$, which is generated by all open intervals in $\\mathbb R$ through countable unions, intersections, and complements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca05842",
   "metadata": {},
   "source": [
    "\n",
    "### **`Def. 2.3`: Distribution (law) of a Random Variable**\n",
    "Given a probability space $(\\Omega,\\mathcal F,P)$, a measurable space $(\\mathbb Y,\\mathcal Y)$, and a random variable $X$, its distribution (law) is defined as a **pushforward probability measure** $P_X$\n",
    "\n",
    "**Meaning**:\n",
    "\n",
    "$$\n",
    "\\forall B\\in(\\mathbb Y,\\mathcal Y) \\text{, we have } P_X(B) = P(X\\in B)=P(X^{-1}(B)).\n",
    "$$\n",
    "\n",
    "\n",
    "> * **Important note**: having the same distribution does **not** mean two random variables are equal as functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699c0529",
   "metadata": {},
   "source": [
    "## `Distributions and Probability Mass Functions (PMFs)`\n",
    "\n",
    "* There are 2 main types of distributions: ***discrete*** and ***continuous***.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b691bb",
   "metadata": {},
   "source": [
    "## `3.1 Discrete distributions`\n",
    "\n",
    "#### **`Def. 3.2.1`: Discrete random variable**\n",
    "\n",
    "A random variable $X$ is said to be **discrete** if it takes values in a finite or countably infinite set\n",
    "$$\n",
    "\\{a_1,a_2,\\dots\\}\n",
    "\\quad\\text{such that}\\quad\n",
    "P(X=a_j\\text{ for some }j)=1.\n",
    "$$\n",
    "\n",
    "If $X$ is a discrete random variable, then this finite or countable set of values such that $P(X = x) > 0$ is called the **support** of $X$.\n",
    "\n",
    "\n",
    "> * **Note**: ***Continuous Random Variables*** can take any value in an interval.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb3ddb4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "* The *distribution* of a **Random Variable** specifies the probabilities of all events addociated with it. For a discrete random variable, this is captured by the **Probability Mass Function (PMF)**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e165f41",
   "metadata": {},
   "source": [
    "#### **`Def. 3.2.2`: Probability Mass Function**\n",
    "\n",
    "The **probability mass function (PMF)** of a discrete random variable $X$ is the function $p_X$ given by:\n",
    "$$\n",
    "p_X(x)=P(X=x).\n",
    "$$\n",
    "It is $>0$ on the support of $X$ and $0$ otherwise.\n",
    "\n",
    ">* **Notes**: In writing $P(X=x)$, we mean the probability of the **event** $\\{\\omega\\in\\Omega: X(\\omega)=x\\}$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dad3518",
   "metadata": {},
   "source": [
    "#### **`Thm. 3.2.7`: Valid PMFs**\n",
    "Let $X$ be a discrete random variable with support $\\{x_1,x_2,\\dots\\}$. The *Probability Mass Function* (PMF) $p_X$ of $X$ must satisfy:\n",
    "\n",
    "1. **Non-negativity**: \n",
    "$$p_X(x)\\ge 0 \\text{ for all } x = x_j, \\text{ for some } j. \\quad p_X(x) = 0 \\text{ otherwise.}$$\n",
    "\n",
    "2. **Normalization**: \n",
    "$$\n",
    "\\sum_{j=1}^\\infty p_X(x_j)=1\n",
    "$$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba129e8d",
   "metadata": {},
   "source": [
    "## `3.2 Cumulative Distribution Functions (CDF)` \n",
    "\n",
    "* Works for all random variables.\n",
    "\n",
    "---\n",
    "\n",
    "#### **`Def. 3.6.1`: Cumulative distribution function (CDF)**\n",
    "\n",
    "The **Cumulative distribution function** (CDF) of a random variable $X$ is the function $F_X$ given by:\n",
    "$$\n",
    "F_X(x)=P(X\\le x).\n",
    "$$\n",
    "\n",
    ">* **Note**: Only *discrete random variables* have **Probability Mass Functions** (PMFs), but **all random variables have CDFs**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48a087b",
   "metadata": {},
   "source": [
    "#### **`Thm. 3.6.3`: Valid CDFs**\n",
    "Any Cumulative Distribution Function (CDF) $F$ has these properties:\n",
    "\n",
    "1) **Increasing**:\n",
    "$$\n",
    "\\text{If } x_1\\le x_2 \\text{ then } F(x_1)\\le F(x_2)\n",
    "$$\n",
    "\n",
    "2) **Right-continuous**: The CFD is **Continuous**, except for having some jumps. At the point of a jump, the CDF is continuous from the **right** $\\forall a\\in\\mathbb R$:\n",
    "$$\n",
    "F(a)=\\lim_{x\\to a^+}F(x)\n",
    "$$\n",
    "\n",
    "3) **Convergence to $0$ and $1$**:\n",
    "$$\n",
    "\\lim_{x\\to-\\infty}F(x)=0 \\text{   and   } \\lim_{x\\to\\infty}F(x)=1\n",
    "$$\n",
    "\n",
    "4) **Normalization**:\n",
    "$$\n",
    "\\forall x\\in\\mathbb R:\\quad 0\\le F(x)\\le 1 \\text{- Range is bounded}\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a890b146",
   "metadata": {},
   "source": [
    "## `3.3 Relationship between PMFs and CDFs`:\n",
    "\n",
    "For **discrete random variables**, we can easily convert between PMFs and CDFs:\n",
    "\n",
    "1. **From PMF to CDF**:\n",
    "- To find, for example, $P(X\\le x_0)$, (where $x_0$ is some real number), we sum the PMF values for all support points $x_j$ that are $\\le x_0$:\n",
    "\n",
    "$$\n",
    "F_X(x) = P(X\\le x) = \\sum_{x_j \\le x} p_X(x_j)\n",
    "$$\n",
    "\n",
    "2. **From CDF to PMF**:\n",
    "- The CDF of a discrete random variable consists of jumps and flat regions. The ***Height of a jump at x*** = ***Value of the PMF at $x_j$***:\n",
    "$$\n",
    "p_X(x_j) = F_X(x_j) - \\lim_{x\\to x_j^-} F_X(x)\n",
    "$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9c1e54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5472f87a",
   "metadata": {},
   "source": [
    "## `4. Characteristic function (CF) and moment-generating function (MGF)`\n",
    "\n",
    "### **What is it / why do we care?**\n",
    "\n",
    "**Big idea:** Sometimes the CDF/PDF is the “wrong coordinate system” for a problem.  \n",
    "CF and MGF are **transforms** of the distribution that:\n",
    "- compress the whole distribution into a single function,\n",
    "- turn *hard operations* (like convolution / sums of independent RVs) into *easy algebra* (multiplication / addition),\n",
    "- encode moments (when they exist) through derivatives/Taylor expansions,\n",
    "- are powerful for limit theorems (CLT, convergence in distribution), because “convergence of CFs” is a standard tool. \n",
    "\n",
    "---\n",
    "\n",
    "## `4.1 Characteristic function`\n",
    "\n",
    "### **Definition**\n",
    "Let $X$ be a real-valued random variable. Its **characteristic function** is\n",
    "$$\n",
    "\\varphi_X(t) \\;=\\; \\mathbb E\\!\\left[e^{itX}\\right]\n",
    "\\;=\\; \\int_{\\mathbb R} e^{itx}\\, dP_X(x),\n",
    "\\qquad t\\in\\mathbb R.\n",
    "$$ \n",
    "\n",
    "**How to interpret it (intuition):**\n",
    "- The kernel $e^{itx}$ lies on the unit circle in $\\mathbb C$.\n",
    "- So $\\varphi_X(t)$ is like a **weighted “average rotation”** of unit vectors $e^{itx}$ under the distribution of $X$.\n",
    "- This is a **Fourier transform of the probability measure** $P_X$, i.e. you’re viewing the distribution in a “frequency domain”.\n",
    "\n",
    "### **Key properties (what they *mean*)**\n",
    "1. **Always exists:** for every $X$ and every $t\\in\\mathbb R$, $\\varphi_X(t)$ is defined (because $|e^{itX}|=1$). \n",
    "2. **Bounded:** \n",
    "$$\n",
    "|\\varphi_X(t)|\\le 1.\n",
    "$$\n",
    "(You’re averaging points on the unit circle, so you can’t get magnitude bigger than 1.) \n",
    "3. **Normalization:** \n",
    "$$\n",
    "\\varphi_X(0)=1.\n",
    "$$\n",
    "(At $t=0$, the kernel is $1$ everywhere, so the average is $1$.)  \n",
    "4. **Uniqueness / injectivity:** $\\varphi_X$ determines the distribution $P_X$ (you can recover $P_X$ via an inverse Fourier-type result).\n",
    "5. **Continuity:** $\\varphi_X$ is (uniformly) continuous in $t$.\n",
    "\n",
    "### **The main “why it’s useful”: sums become products**\n",
    "If $X$ and $Y$ are independent, then\n",
    "$$\n",
    "\\varphi_{X+Y}(t) = \\varphi_X(t)\\,\\varphi_Y(t).\n",
    "$$\n",
    "So:\n",
    "- adding independent variables (a convolution in the PDF world) becomes **multiplication** in CF world,\n",
    "- and for i.i.d. sums $S_n=\\sum_{k=1}^n X_k$:\n",
    "$$\n",
    "\\varphi_{S_n}(t) = \\big(\\varphi_X(t)\\big)^n.\n",
    "$$\n",
    "This is one of the reasons CFs are central in proofs of CLT and other limit results.  \n",
    "\n",
    "### **Moments from derivatives (when moments exist)**\n",
    "If $\\mathbb E[|X|^n]<\\infty$, then derivatives at $0$ encode moments:\n",
    "$$\n",
    "\\varphi_X^{(n)}(0) = i^n\\,\\mathbb E[X^n].\n",
    "$$ \n",
    "\n",
    "Equivalently (formal Taylor expansion near $0$):\n",
    "$$\n",
    "\\varphi_X(t)=\\sum_{n=0}^{\\infty}\\frac{(it)^n}{n!}\\,\\mathbb E[X^n]\n",
    "\\quad\\text{(when the expansion is justified).}\n",
    "$$ \n",
    "\n",
    "---\n",
    "\n",
    "## `4.2 Moment-generating function (MGF)`\n",
    "\n",
    "### **Definition**\n",
    "The **moment-generating function** is\n",
    "$$\n",
    "M_X(t) \\;=\\; \\mathbb E[e^{tX}]\n",
    "\\;=\\; \\int_{\\mathbb R} e^{tx}\\, dP_X(x),\n",
    "\\qquad t\\in\\mathbb R \\text{ (where finite).}\n",
    "$$\n",
    "\n",
    "**Key intuition:**\n",
    "- Compare kernels: CF uses $e^{itX}$ (bounded), MGF uses $e^{tX}$ (can explode).\n",
    "- So MGF “sees” **tail growth** very strongly: large positive values of $X$ get exponentially amplified when $t>0$. \n",
    "\n",
    "### **Important warning: MGF may not exist**\n",
    "Because $e^{tX}$ is unbounded, $M_X(t)$ can be infinite or undefined for some (or all) $t\\ne 0$.\n",
    "Usually we require: **there exists an open interval around $0$ where $M_X(t)<\\infty$**.\n",
    "\n",
    "### **Moments from derivatives**\n",
    "If $M_X(t)$ is finite in a neighborhood of $0$, then:\n",
    "$$\n",
    "M_X^{(n)}(0) = \\mathbb E[X^n].\n",
    "$$\n",
    "\n",
    "So MGF is literally a “moment machine”: the Taylor coefficients at $0$ are raw moments:\n",
    "$$\n",
    "M_X(t)=\\sum_{n=0}^{\\infty}\\frac{t^n}{n!}\\,\\mathbb E[X^n]\n",
    "\\quad\\text{(when analytic near 0).}\n",
    "$$\n",
    "\n",
    "### **Sums of independent RVs**\n",
    "If $X$ and $Y$ are independent and MGFs exist where needed:\n",
    "$$\n",
    "M_{X+Y}(t)=M_X(t)\\,M_Y(t).\n",
    "$$\n",
    "Same “convolution becomes multiplication” advantage as CF.\n",
    "\n",
    "### **CF vs MGF in one line**\n",
    "- CF: always exists, great for distribution/limits, moments via $i^n$ derivatives.\n",
    "- MGF: may fail to exist, but when it exists it’s very convenient for moments and exponential tail behavior.\n",
    "\n",
    "---\n",
    "\n",
    "# `5. Moments and cumulants`\n",
    "\n",
    "## `5.1 Moments`\n",
    "\n",
    "### **What is it / why do we care?**\n",
    "**Moments** summarize a distribution numerically:\n",
    "- mean = “center of mass” (balance point),\n",
    "- variance = “spread around the mean” (squared-distance style),\n",
    "- skewness = “asymmetry / tilt”,\n",
    "- kurtosis = “tail heaviness / peakiness”.\n",
    "\n",
    "They’re used to:\n",
    "- compare distributions,\n",
    "- approximate distributions (moment matching),\n",
    "- detect tail risk (fat tails),\n",
    "- build estimators/statistics.\n",
    "\n",
    "### **Raw moments**\n",
    "The $n$-th **raw moment** is\n",
    "$$\n",
    "m_n := \\mathbb E[X^n].\n",
    "$$\n",
    "\n",
    "### **Central moments**\n",
    "Let $\\mu := \\mathbb E[X]$. The $n$-th **central moment** is\n",
    "$$\n",
    "\\mu_n := \\mathbb E[(X-\\mu)^n].\n",
    "$$\n",
    "\n",
    "A key relation: central moments can be written from raw moments by binomial expansion:\n",
    "$$\n",
    "\\mu_n\n",
    "= \\mathbb E[(X-\\mu)^n]\n",
    "= \\sum_{k=0}^n {n\\choose k}(-\\mu)^{\\,n-k}\\,m_k.\n",
    "$$ \n",
    "\n",
    "### **Most used ones**\n",
    "1. **Mean**\n",
    "$$\n",
    "\\mu = \\mathbb E[X] = m_1.\n",
    "$$\n",
    "\n",
    "2. **Variance**\n",
    "$$\n",
    "\\mathrm{Var}(X)=\\mu_2=\\mathbb E[(X-\\mu)^2]=\\mathbb E[X^2]-(\\mathbb E[X])^2 = m_2 - m_1^2.\n",
    "$$\n",
    "\n",
    "3. **Skewness (dimensionless)**\n",
    "Let $\\sigma=\\sqrt{\\mu_2}$. Then\n",
    "$$\n",
    "\\gamma_1 := \\frac{\\mu_3}{\\sigma^3}.\n",
    "$$\n",
    "\n",
    "4. **Kurtosis (dimensionless)**\n",
    "$$\n",
    "\\gamma_2 := \\frac{\\mu_4}{\\sigma^4}.\n",
    "$$ \n",
    "\n",
    "### **Caution: moments may not exist**\n",
    "Some distributions look “nice” but have divergent moments (classic example: Cauchy).\n",
    "So “I can compute the mean/variance” is not automatic — it requires integrability assumptions.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## `5.2 Cumulants`\n",
    "\n",
    "### **What is it / why do we care?**\n",
    "Cumulants are another set of numerical summaries, but with a *huge structural advantage*:\n",
    "\n",
    "> **Cumulants add under independent sums.**\n",
    "\n",
    "That makes them extremely useful for:\n",
    "- analyzing sums of independent RVs,\n",
    "- approximation/expansions (Edgeworth / saddlepoint ideas),\n",
    "- separating “shape information” cleanly (mean/variance/skewness/kurtosis appear naturally).  \n",
    "\n",
    "Moments mix together under sums; cumulants behave cleanly.\n",
    "\n",
    "---\n",
    "\n",
    "### **Cumulant generating function (CGF)**\n",
    "Assume the MGF exists near $0$. Define the **cumulant generating function**\n",
    "$$\n",
    "K_X(t) := \\log M_X(t).\n",
    "$$\n",
    "\n",
    "The **$n$-th cumulant** is\n",
    "$$\n",
    "\\kappa_n := K_X^{(n)}(0).\n",
    "$$\n",
    "\n",
    "So cumulants are “derivatives of $\\log$ MGF at 0”.\n",
    "\n",
    "---\n",
    "\n",
    "### **First cumulants (interpretation)**\n",
    "When they exist:\n",
    "- $\\kappa_1 = \\mathbb E[X]$ (mean),\n",
    "- $\\kappa_2 = \\mathrm{Var}(X)$ (variance),\n",
    "- $\\kappa_3 = \\mu_3$ (third central moment),\n",
    "- $\\kappa_4 = \\mu_4 - 3\\mu_2^2$ (related to “excess kurtosis structure”).\n",
    "\n",
    "(So cumulants are tightly linked to central moments; higher-order ones remove the “redundant combinations” that moments contain.)\n",
    "\n",
    "---\n",
    "\n",
    "### **The killer property: additivity**\n",
    "If $X$ and $Y$ are independent and MGFs exist near $0$, then:\n",
    "$$\n",
    "M_{X+Y}(t)=M_X(t)M_Y(t)\n",
    "\\quad\\Rightarrow\\quad\n",
    "K_{X+Y}(t)=K_X(t)+K_Y(t).\n",
    "$$\n",
    "Taking derivatives at $0$ gives:\n",
    "$$\n",
    "\\kappa_n(X+Y)=\\kappa_n(X)+\\kappa_n(Y).\n",
    "$$\n",
    "\n",
    "This is why cumulants are often the right tool whenever you see “sum of independent random variables”.\n",
    "\n",
    "---\n",
    "\n",
    "## What to say in an oral exam (short + correct)\n",
    "\n",
    "**CF:**\n",
    "> “The characteristic function is $\\varphi_X(t)=\\mathbb E[e^{itX}]$, the Fourier transform of $P_X$. It always exists, determines the distribution, and turns sums of independent RVs into products: $\\varphi_{X+Y}=\\varphi_X\\varphi_Y$. Derivatives at $0$ encode moments when they exist.”\n",
    "\n",
    "**MGF:**\n",
    "> “The MGF is $M_X(t)=\\mathbb E[e^{tX}]$, a Laplace-type transform. It may fail to exist, but if it exists near $0$ then $M_X^{(n)}(0)=\\mathbb E[X^n]$ and $M_{X+Y}=M_XM_Y$ for independent sums.”\n",
    "\n",
    "**Moments vs cumulants:**\n",
    "> “Moments summarize shape (mean/variance/skewness/kurtosis) but can mix under sums; cumulants come from $K_X(t)=\\log M_X(t)$ and are additive for independent sums, which makes them especially useful for analyzing sums and approximations.”\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42053e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================================ #\n",
    "# ================================================================================================ #\n",
    "# ================================================================================================ #\n",
    "# ================================================================================================ #\n",
    "# ================================================================================================ #\n",
    "# ================================================================================================ #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610e4eaf",
   "metadata": {},
   "source": [
    "# `Second Lector`:\n",
    "\n",
    "---\n",
    "\n",
    "### **`Def. 2.2` Random Variable**:\n",
    "\n",
    "Given a sample space $(\\Omega,\\mathcal{F})$ and a measurable space $(Y,\\mathcal{Y})$, a function $\\mathcal{F}$-measurable function $X:(\\Omega,\\mathcal{F})\\to(Y,\\mathcal{Y})$ is called a **random variable**.\n",
    "\n",
    "---\n",
    "\n",
    "### **`Def. 2.3` Distribution of a Random Variable**:\n",
    "\n",
    "Given a probability space $(\\Omega,\\mathcal{F},P)$, a measurable space $(\\mathbb{Y},\\mathcal{Y})$, and a random variable $X$, its distribution (law) is defined as a **pushforward probability measure** $P_X$ on $(\\mathbb{Y},\\mathcal{Y})$ by:\n",
    "$$\\forall B\\in\\mathcal{Y}:\\quad P_X(B) = P(X\\in B) = P(X^{-1}(B)).$$\n",
    "\n",
    "---\n",
    "\n",
    "### **`Def. 2.4` Cumulative Distribution Function (CFD)**\n",
    "\n",
    "For all given probability space $(\\Omega,\\mathcal{F},P)$ and random variable $X$ together with its distribution $P_X$ and a number $x\\in\\mathbb{R}$, the **Cumulative Distribution Function** $F_X :\\mathbb{R}\\to[0,1]$ is defined as:\n",
    "\n",
    "$$\n",
    "F_X(x) = P(X^{-1}(-\\infty,x]) = P_X((-\\infty,x]) \n",
    "$$\n",
    "\n",
    "Which satisfies:\n",
    "\n",
    "1. **Non-decreasing**: \n",
    "$$\n",
    "\\text{if } x_1\\le x_2 \\text{ then } F_X(x_1)\\le F_X(x_2)\n",
    "$$\n",
    "\n",
    "2. **Right-continuous**:\n",
    "$$\n",
    "F_X(a) = \\lim_{x\\to a^+} F_X(x)\n",
    "$$\n",
    "\n",
    "3. **Limits at infinity**:\n",
    "$$\n",
    "\\lim_{x\\to -\\infty} F_X(x) = 0 \\quad\\text{and}\\quad \\lim_{x\\to +\\infty} F_X(x) = 1\n",
    "$$\n",
    "\n",
    "4. **Normalization**:\n",
    "$$\n",
    "0 \\le F_X(x) \\le 1 \\text{ for all } x\\in\\mathbb{R}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **`Def. 2.5` Probability Mass Function (PMF)**:\n",
    "\n",
    "Given a probability space $(\\Omega,\\mathcal{F},P)$ and a discrete random variable $X$ taking values on a countable set $S=\\{x_1,x_2,\\dots\\}$, the **Probability Mass Function (PMF)** of $X$ is a function $p_X:\\mathbb{R}\\to[0,1]$ is defined as:\n",
    "\n",
    "$$\n",
    "p_X(x) = P(X=x) \\text{ for all } x\\in S\n",
    "$$\n",
    "\n",
    "With the properties:\n",
    "\n",
    "1. **Non-negativity**: \n",
    "$$\n",
    "p_X(x) \\ge 0 \\text{ for all } x\\in S\n",
    "$$\n",
    "\n",
    "2. **Normalization**:\n",
    "$$\n",
    "\\sum_{x\\in S} p_X(x) = 1\n",
    "$$\n",
    "\n",
    "3. **Zero outside support**:\n",
    "$$\n",
    "p_X(x) = 0 \\text{ if } x\\notin S\n",
    "$$\n",
    "\n",
    "> **Note**: Reconvering CDF: $$F_X(x) = \\sum_{x_j\\le x} p_X(x_j)$$\n",
    "\n",
    "---\n",
    "\n",
    "### **`Def. 2.6` Probability Density Function (PDF)**:\n",
    "\n",
    "Given a probability space $(\\Omega,\\mathcal{F},P)$ and a continuous real valued random variable $X$ with distribution $P_X \\le \\lambda$ and a $F_X$ for CDF, a **Probability Density Function (PDF)** is a function $f_X:\\mathbb{R}\\to[0,\\infty)$ where:\n",
    "\n",
    "1. \n",
    "$$\n",
    "f_X(x) \\ge 0 \\text{ for all } x\\in\\mathbb{R}\n",
    "$$\n",
    "\n",
    "2. \n",
    "$$\n",
    "\\int_{-\\infty}^{\\infty} f_X(x) d(x) = 1\n",
    "$$\n",
    "\n",
    "\n",
    "3.\n",
    "$$\n",
    "P(X\\in B) = \\int_B f_X(x) d(x) \n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0578d8fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a6938cb",
   "metadata": {},
   "source": [
    "# `Second Lector`:\n",
    "\n",
    "---\n",
    "\n",
    "### **`Def. 2.7` Expected Value ($E$ / Mean $\\mu$ )**:\n",
    "\n",
    "Given a probability space $(\\Omega,\\mathcal{F},P)$ and a random variable $X$, the **Expected Value** (or mean) is the weighted average of all possible values of $X$. It is defined as:\n",
    "\n",
    "1. **Discrete Case**: \n",
    "$$E[X] = \\sum_{i} x_i P(X = x_i)$$\n",
    "2. **Continuous Case**: \n",
    "$$E[X] = \\int_{-\\infty}^{\\infty} x f_X(x) dx$$\n",
    "\n",
    "**Properties**:\n",
    "* **Linearity**: $E[aX + bY] = aE[X] + bE[Y]$ for constants $a, b$.\n",
    "* **Expectation of a function ($X \\to g(X)$)**: $E[g(X)] = \\int g(x) f_X(x) dx$.\n",
    "\n",
    "Where $f_X(x)$ is the probability density function of $X$.\n",
    "\n",
    "---\n",
    "\n",
    "### **`Def. 2.8` Variance ($\\sigma^2$)**:\n",
    "\n",
    "The **Variance** of a random variable $X$ measures the spread of the distribution around its mean $\\mu = E[X]$. It is defined as:\n",
    "\n",
    "$$Var(X) = E[(X - E[X])^2]$$\n",
    "\n",
    "**Properties**:\n",
    "1. **Alternative Formula**: $Var(X) = E[X^2] - (E[X])^2$\n",
    "2. **Non-negativity**: $Var(X) \\ge 0$\n",
    "3. **Scaling**: $Var(aX + b) = a^2 Var(X)$\n",
    "\n",
    "---\n",
    "\n",
    "### **`Def. 2.9` Covariance ($Cov$)**:\n",
    "\n",
    "Given two random variables $X$ and $Y$ defined on the same probability space, the **Covariance** measures their joint linear variability:\n",
    "\n",
    "$$Cov(X, Y) = E[(X - E[X])(Y - E[Y])]$$\n",
    "\n",
    "**Properties**:\n",
    "1. **Alternative Formula**: $Cov(X, Y) = E[XY] - E[X]E[Y]$\n",
    "2. **Symmetry**: $Cov(X, Y) = Cov(Y, X)$\n",
    "3. **Relation to Variance**: $Cov(X, X) = Var(X)$\n",
    "4. **Variance of a Sum**: $Var(X + Y) = Var(X) + Var(Y) + 2Cov(X, Y)$\n",
    "\n",
    "---\n",
    "\n",
    "### **`Def. 2.10` Independence of Random Variables**:\n",
    "\n",
    "Two random variables $X$ and $Y$ are **independent** if for all measurable sets $A, B$, the events $\\{X \\in A\\}$ and $\\{Y \\in B\\}$ are independent. This implies:\n",
    "\n",
    "$$P(X \\in A, Y \\in B) = P(X \\in A)P(Y \\in B)$$\n",
    "\n",
    "**Properties under Independence**:\n",
    "1. $E[XY] = E[X]E[Y]$\n",
    "2. $Cov(X, Y) = 0$\n",
    "3. $Var(X + Y) = Var(X) + Var(Y)$\n",
    "\n",
    "---\n",
    "\n",
    "### **`Def. 2.11` Marginal Distributions**:\n",
    "\n",
    "Given a joint distribution (PDF $f_{X,Y}$ or PMF $p_{X,Y}$) of two random variables $X$ and $Y$, the **Marginal Distribution** of one variable is obtained by \"summing out\" or \"integrating out\" the other variable.\n",
    "\n",
    "1. **Marginal PMF (Discrete)**: \n",
    "$$p_X(x) = \\sum_{y \\in S_y} p_{X,Y}(x, y)$$\n",
    "2. **Marginal PDF (Continuous)**: \n",
    "$$f_X(x) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x, y) dy$$\n",
    "\n",
    "> **Note**: The marginal distribution provides the probability behavior of $X$ ignoring all information about $Y$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece86e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e907d97",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **`Def. 2.12` Random Variable (The Source)**:\n",
    "\n",
    "A **Random Variable (RV)** is a numerical description of the outcome of a statistical experiment. It maps outcomes from a sample space to real numbers.\n",
    "*   **Discrete RV**: Values you can count (e.g., $0, 1, 2$).\n",
    "*   **Continuous RV**: Values in a range (e.g., $1.75\\dots$ meters).\n",
    "\n",
    "---\n",
    "\n",
    "### **`Def. 2.13` Probability Mass Function (PMF) - [Discrete Only]**:\n",
    "\n",
    "For discrete random variables, the **PMF** gives the probability that the variable is exactly equal to some value.\n",
    "\n",
    "$$p_X(x) = P(X = x)$$\n",
    "\n",
    "**Key Rules**:\n",
    "1. All probabilities are between 0 and 1.\n",
    "2. The sum of all probabilities must equal 1: $\\sum p_X(x) = 1$.\n",
    "\n",
    "---\n",
    "\n",
    "### **`Def. 2.14` Probability Density Function (PDF) - [Continuous Only]**:\n",
    "\n",
    "For continuous random variables, the probability of hitting a *single exact point* is zero ($P(X=1.5) = 0$). Instead, we use the **PDF** to find probability over an **interval**.\n",
    "\n",
    "$$P(a \\le X \\le b) = \\int_{a}^{b} f_X(x) dx$$\n",
    "\n",
    "**Key Rules**:\n",
    "1. $f_X(x) \\ge 0$ (Density cannot be negative).\n",
    "2. The total area under the curve must be 1: $\\int_{-\\infty}^{\\infty} f_X(x) dx = 1$.\n",
    "\n",
    "---\n",
    "\n",
    "### **`Def. 2.15` Cumulative Distribution Function (CDF) - [Universal]**:\n",
    "\n",
    "The **CDF** is the most powerful tool because it applies to *both* discrete and continuous variables. It calculates the \"running total\" of probability up to a point $x$.\n",
    "\n",
    "$$F_X(x) = P(X \\le x)$$\n",
    "\n",
    "**The Relationship \"The Calculus of Stats\"**:\n",
    "*   **To get CDF from PDF**: Integrate. $F_X(x) = \\int_{-\\infty}^{x} f_X(t) dt$.\n",
    "*   **To get PDF from CDF**: Differentiate. $f_X(x) = \\frac{d}{dx} F_X(x)$.\n",
    "\n",
    "---\n",
    "\n",
    "### **`Def. 2.16` Joint, Marginal, and Conditional Distributions**:\n",
    "\n",
    "When studying two variables ($X$ and $Y$) simultaneously:\n",
    "\n",
    "1.  **Joint Distribution**: The probability that $X$ and $Y$ happen at the same time.\n",
    "    *   $f_{X,Y}(x,y)$\n",
    "2.  **Marginal Distribution**: The distribution of just $X$, ignoring $Y$.\n",
    "    *   Find it by \"collapsing\" $Y$: $f_X(x) = \\int f_{X,Y}(x,y) dy$.\n",
    "3.  **Conditional Distribution**: The distribution of $X$ *given* that we know $Y$ has already happened.\n",
    "    *   $f_{X|Y}(x|y) = \\frac{f_{X,Y}(x,y)}{f_Y(y)}$\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary for Memorization**:\n",
    "\n",
    "| Term | Symbol | Question it Answers | Data Type |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **PMF** | $p(x)$ | \"What is the chance of exactly $x$?\" | Discrete |\n",
    "| **PDF** | $f(x)$ | \"How dense is the probability near $x$?\" | Continuous |\n",
    "| **CDF** | $F(x)$ | \"What is the chance of getting $x$ or less?\" | Both |\n",
    "| **Joint** | $f(x,y)$ | \"What is the chance of $x$ AND $y$?\" | Both |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0a50f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c179b19c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **`Def. 2.17` Moments of a Random Variable**:\n",
    "\n",
    "Moments are a set of statistical measures used to describe the shape of a distribution.\n",
    "1.  **1st Moment**: $E[X]$ (Mean) - Location of the center.\n",
    "2.  **2nd Central Moment**: $E[(X-\\mu)^2]$ (Variance) - Width of the spread.\n",
    "3.  **3rd Central Moment**: **Skewness** - Measures asymmetry (leaning left or right).\n",
    "4.  **4th Central Moment**: **Kurtosis** - Measures \"tailedness\" (how many outliers exist).\n",
    "\n",
    "---\n",
    "\n",
    "### **`Def. 2.18` Law of the Unconscious Statistician (LOTUS)**:\n",
    "\n",
    "A critical theorem used to calculate the expected value of a **function** of a random variable without needing to find the distribution of $g(X)$ first:\n",
    "\n",
    "$$E[g(X)] = \\int_{-\\infty}^{\\infty} g(x) f_X(x) dx \\quad \\text{(Continuous)}$$\n",
    "$$E[g(X)] = \\sum_{x} g(x) p_X(x) \\quad \\text{(Discrete)}$$\n",
    "\n",
    "---\n",
    "\n",
    "### **`Def. 2.19` Moment Generating Function (MGF)**:\n",
    "\n",
    "The **MGF** is a functional \"DNA\" of a random variable. If two variables have the same MGF, they have the same distribution. It is defined as:\n",
    "\n",
    "$$M_X(t) = E[e^{tX}]$$\n",
    "\n",
    "**Why it matters**: \n",
    "To find the $n$-th moment, you simply take the $n$-th derivative of $M_X(t)$ and evaluate it at $t=0$:\n",
    "$$E[X^n] = M_X^{(n)}(0)$$\n",
    "\n",
    "---\n",
    "\n",
    "### **`Def. 2.20` Correlation ($\\rho$)**:\n",
    "\n",
    "While **Covariance** tells you the direction of a relationship, it is scale-dependent. **Correlation** is the \"standardized\" version that scales the relationship between $-1$ and $+1$.\n",
    "\n",
    "$$\\rho_{X,Y} = \\text{Corr}(X,Y) = \\frac{Cov(X,Y)}{\\sigma_X \\sigma_Y}$$\n",
    "\n",
    "*   **$\\rho = 1$**: Perfect positive linear relationship.\n",
    "*   **$\\rho = -1$**: Perfect negative linear relationship.\n",
    "*   **$\\rho = 0$**: No linear relationship.\n",
    "\n",
    "---\n",
    "\n",
    "### **`Def. 2.21` Standard Deviation ($\\sigma$)**:\n",
    "\n",
    "The **Standard Deviation** is simply the square root of the variance. We use it because it returns the \"spread\" measurement back to the **original units** of the data.\n",
    "\n",
    "$$\\sigma = \\sqrt{Var(X)}$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary of Important Inequalities**:\n",
    "\n",
    "1.  **Markov's Inequality**: Provides an upper bound for the probability that a non-negative RV is greater than a constant.\n",
    "    $$P(X \\ge a) \\le \\frac{E[X]}{a}$$\n",
    "2.  **Chebyshev's Inequality**: Guarantees that nearly all values are close to the mean (used for outlier detection).\n",
    "    $$P(|X - \\mu| \\ge k\\sigma) \\le \\frac{1}{k^2}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bbd052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3af5c6c",
   "metadata": {},
   "source": [
    "# `Third Lector`: Advanced Topics in Random Variables\n",
    "\n",
    "---\n",
    "\n",
    "### **`Topic 3.1` Families of Distributions**:\n",
    "\n",
    "**Families of Distributions** are standardized probability models that describe common real-world phenomena. They have pre-defined parameters that determine their shape, center, and spread.\n",
    "\n",
    "#### A. Discrete Families\n",
    "\n",
    "| Name | Notation | Parameters | Common Use |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Bernoulli** | $Bern(p)$ | $p \\in [0, 1]$ | Single coin flip outcome (Success/Failure). |\n",
    "| **Binomial** | $Bin(n, p)$ | $n \\in \\mathbb{N}, p \\in [0, 1]$ | Count of successes in $n$ independent trials. |\n",
    "| **Poisson** | $Pois(\\lambda)$ | $\\lambda > 0$ | Count of rare events over a fixed time/space interval (e.g., calls per hour). |\n",
    "\n",
    "#### B. Continuous Families\n",
    "\n",
    "| Name | Notation | Parameters | Common Use |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Uniform** | $U(a, b)$ | $a, b \\in \\mathbb{R}$ | Events equally likely to occur within an interval. |\n",
    "| **Exponential** | $Exp(\\lambda)$ | $\\lambda > 0$ | Time until the next event occurs (memoryless property). |\n",
    "| **Normal (Gaussian)** | $N(\\mu, \\sigma^2)$ | $\\mu \\in \\mathbb{R}, \\sigma^2 > 0$ | Central Limit Theorem; most common natural distribution (heights, errors). |\n",
    "\n",
    "---\n",
    "\n",
    "### **`Topic 3.2` Statistics**:\n",
    "\n",
    "In this context, a **Statistic** is any function of the observable data (a sample) that does not depend on unknown population parameters. It is itself a random variable.\n",
    "\n",
    "| Term | Definition | Example |\n",
    "| :--- | :--- | :--- |\n",
    "| **Statistic** | A quantity calculated from sample data used to estimate a population parameter. | Sample Mean ($\\bar{x}$), Sample Variance ($s^2$). |\n",
    "| **Parameter** | A fixed, unknown value describing the entire population. | Population Mean ($\\mu$), Population Variance ($\\sigma^2$). |\n",
    "| **Estimator** | A specific type of statistic used to estimate a parameter. | $\\bar{x}$ is an estimator for $\\mu$. |\n",
    "| **Bias** | The difference between the expected value of an estimator and the true parameter value: $Bias(\\hat{\\theta}) = E[\\hat{\\theta}] - \\theta$. |\n",
    "| **Efficiency** | The inverse of the variance of an estimator (lower variance is better). |\n",
    "\n",
    "---\n",
    "\n",
    "### **`Topic 3.3` Entropy ($H$)**:\n",
    "\n",
    "**Entropy** in information theory measures the average level of \"information,\" \"surprise,\" or uncertainty inherent in a random variable's possible outcomes.\n",
    "\n",
    "*   **Higher Entropy**: More uncertainty/randomness (e.g., flipping a fair coin).\n",
    "*   **Lower Entropy**: Less uncertainty/more predictability (e.g., a coin weighted to land on heads 99% of the time).\n",
    "\n",
    "#### Definitions:\n",
    "*   **Discrete Entropy (Shannon Entropy)**: \n",
    "    $$H(X) = -E[\\log P(X)] = -\\sum_{x} P(x) \\log P(x)$$\n",
    "*   **Differential Entropy (Continuous)**:\n",
    "    $$H(X) = -\\int_{-\\infty}^{\\infty} f_X(x) \\log f_X(x) dx$$\n",
    "\n",
    "---\n",
    "\n",
    "### **`Topic 3.4` Independent Random Variables, Marginals**:\n",
    "\n",
    "These topics link back to concepts covered in the previous Lector notes but emphasize their role in multivariate analysis:\n",
    "\n",
    "#### A. Independence Review\n",
    "$X$ and $Y$ are independent ($X \\perp Y$) if their joint probability factors into the product of their individual (marginal) probabilities:\n",
    "$$P(X \\in A, Y \\in B) = P(X \\in A)P(Y \\in B)$$\n",
    "\n",
    "#### B. Marginal Distributions Review\n",
    "The marginal distribution of one variable is derived from a joint distribution by integrating or summing over all possible values of the other variable:\n",
    "\n",
    "$$f_X(x) = \\int f_{X,Y}(x, y) dy$$\n",
    "\n",
    "**Key Takeaway**: If you know $X$ and $Y$ are independent, finding their marginals is trivial because $f_{X,Y}(x,y) = f_X(x) \\cdot f_Y(y)$.\n",
    "\n",
    "---\n",
    "\n",
    "### **`Topic 3.5` Characteristic Function (CF) and Moment-Generating Function (MGF)**:\n",
    "\n",
    "These are alternative ways to characterize a distribution fully. The **Characteristic Function ($\\phi_X(t)$)** is often preferred theoretically because it *always* exists, unlike the MGF.\n",
    "\n",
    "*   **Moment-Generating Function (MGF)**:\n",
    "    $$M_X(t) = E[e^{tX}]$$\n",
    "    Used to find moments: $E[X^n] = M_X^{(n)}(0)$.\n",
    "*   **Characteristic Function (CF)**:\n",
    "    $$\\phi_X(t) = E[e^{itX}]$$\n",
    "    Involves imaginary numbers ($i^2 = -1$). It uniquely determines the distribution and is fundamental for proofs using the Central Limit Theorem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474ddc29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
