{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79595de6",
   "metadata": {},
   "source": [
    "# 5. Inequalities and Limit Theorems \n",
    "\n",
    "1. ~~Convexity.~~\n",
    "2. ~~Gibbs’, Markov’s, and Chebyshev’s inequalities.~~\n",
    "3. Modes of convergence. \n",
    "4. ~~Weak and strong laws of large numbers.~~\n",
    "5. ~~Central limit theorem.~~\n",
    "6. ~~Kullback–Leibler divergence, cross-entropy, and mutual information.~~\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4401848d",
   "metadata": {},
   "source": [
    "## `1. Convexity`:\n",
    "\n",
    "### **`Def. 1` Convex Set**:\n",
    "A set $S \\subseteq \\mathbb{R}^n$ is **convex** if for any $x, y \\in S$ and any $\\theta \\in [0, 1]$, the point \n",
    "$$\n",
    "\\theta x + (1 - \\theta) y \\in S\n",
    "$$\n",
    "\n",
    "> * **Meaning**: For any two points in the set, the line segment connecting them lies entirely within the set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d98e0a",
   "metadata": {},
   "source": [
    "### **`Def. 2` Convex Function**:\n",
    "A function $f: \\mathbb{R}^n \\to \\mathbb{R}$ is **convex** if its domain $Dom(f)$ is a convex set and for all $x, y \\in Dom(f)$ and any $\\forall \\theta \\in [0, 1]$, the following inequality holds:\n",
    "$$\n",
    "f(\\theta x + (1 - \\theta) y) \\leq \\theta f(x) + (1 - \\theta) f(y)\n",
    "$$\n",
    "\n",
    "> * **Meaning**: The line segment connecting any two points on the graph of the function lies above or on the graph itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3612fc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8a8bdc7",
   "metadata": {},
   "source": [
    "\n",
    "## `3. Modes of convergence` *(supplement — not present in this PDF)*\n",
    "\n",
    "### Motivation\n",
    "In probability we have several different notions of “$X_n$ approaches $X$”.\n",
    "They are NOT equivalent, so it’s important to know definitions + implication arrows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe1a013",
   "metadata": {},
   "source": [
    "\n",
    "### `Def. (standard) Almost sure convergence`\n",
    "$$\n",
    "X_n \\to X \\ \\text{a.s.}\n",
    "\\quad \\Longleftrightarrow \\quad\n",
    "P\\left(\\{\\omega:\\lim_{n\\to\\infty}X_n(\\omega)=X(\\omega)\\}\\right)=1.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce78cec",
   "metadata": {},
   "source": [
    "\n",
    "### `Def. (standard) Convergence in probability`\n",
    "$$\n",
    "X_n \\xrightarrow{P} X\n",
    "\\quad \\Longleftrightarrow \\quad\n",
    "\\forall \\varepsilon>0:\\ \\ P(|X_n-X|>\\varepsilon)\\to 0.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0715cb",
   "metadata": {},
   "source": [
    "\n",
    "### `Def. (standard) $L^p$ convergence`\n",
    "For $p\\ge 1$:\n",
    "$$\n",
    "X_n \\xrightarrow{L^p} X\n",
    "\\quad \\Longleftrightarrow \\quad\n",
    "\\mathbb E\\big[|X_n-X|^p\\big]\\to 0.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424661d5",
   "metadata": {},
   "source": [
    "### `Def. (standard) Convergence in distribution`\n",
    "$$\n",
    "X_n \\xrightarrow{d} X\n",
    "\\quad \\Longleftrightarrow \\quad\n",
    "F_{X_n}(x)\\to F_X(x) \\ \\ \\text{for all continuity points of }F_X.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00ce51f",
   "metadata": {},
   "source": [
    "### `Standard implication chain (important to memorise)`\n",
    "$$\n",
    "X_n \\xrightarrow{a.s.} X\n",
    "\\ \\Rightarrow\\\n",
    "X_n \\xrightarrow{P} X\n",
    "\\ \\Rightarrow\\\n",
    "X_n \\xrightarrow{d} X.\n",
    "$$\n",
    "\n",
    "Also:\n",
    "$$\n",
    "X_n \\xrightarrow{L^p} X \\ \\Rightarrow\\ X_n \\xrightarrow{P} X.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff31f744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f9a03ef",
   "metadata": {},
   "source": [
    "# `6. Kullback–Leibler Divergence, cross-entropy, and mutual information`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0f65e6",
   "metadata": {},
   "source": [
    "## `Kullback–Leibler Divergence:`\n",
    "\n",
    "\n",
    "Entropy works fine when measuring uncertainty within a single distribution, but what if we want to measure the difference between two distributions?\n",
    "If we have two distributions $P$ and $Q$ over the same set of events, how can we quantify how different they are?\n",
    "The natural way is to compare their ratio:\n",
    "$$\n",
    "\\text{Ralative likelihood} = \\frac{P(x)}{Q(x)}\n",
    "$$\n",
    "\n",
    "Beucase probability measures themselves are scale-free and live in an abstract measure space. The RN-derivative simply tells us how to re-scale one measure to get the other.\n",
    "\n",
    "Holding onto entropy as a measure of surpise, what exactly is the surprize we experience?\n",
    "\n",
    "If reality behaves like $P$ but we expect it to behave like $Q$?\n",
    "\n",
    "We define an average surprise under $P$:\n",
    "$$\n",
    "E_P\\left[-\\log Q(x)\\right] = -\\sum_x P(x)\\log Q(x)\n",
    "$$\n",
    "\n",
    "And to find out hoe much of the surprise we owe to using the wrong model, we take the difference between this and the entropy of $P$:\n",
    "$$\n",
    "E_P\\left[-\\log Q(x)\\right] - E_P\\left[-\\log P(x)\\right] = -\\sum_x P(x)\\log Q(x) + \\sum_x P(x)\\log P(x) = \\sum_x P(x)\\log\\frac{P(x)}{Q(x)}\n",
    "$$\n",
    "$$\n",
    "E_{P}\\left[\\log\\left(\\frac{dP}{dQ}\\right)\\right] = \\sum_x P(x)\\log\\frac{P(x)}{Q(x)}\n",
    "$$\n",
    "\n",
    "This is the **Kullback-Leibler divergence**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280101a2",
   "metadata": {},
   "source": [
    "### **`Def. 10.` KL Divergence**:\n",
    "\n",
    "Let $(\\Omega, \\mathcal{F})$ be a measurable space and let $P$ and $Q$ be two probability measures on this space such that $P$ is absolutely continuous with respect to $Q$ (denoted as $P \\ll Q$). The Kullback-Leibler (KL) divergence from $Q$ to $P$ is defined as:\n",
    "$$\n",
    "D_{KL}(P \\| Q) = \\int_{\\Omega} \\log\\left(\\frac{dP}{dQ}\\right) dP = E_{P}\\left[\\log\\left(\\frac{dP}{dQ}\\right)\\right]\n",
    "$$\n",
    "where $\\frac{dP}{dQ}$ is the Radon-Nikodym derivative of $P$ with respect to $Q$.\n",
    "\n",
    "> * **Interpretation**: KL divergence measures how one probability distribution diverges from a second, expected probability distribution. It is not symmetric, meaning that generally $D_{KL}(P \\| Q) \\neq D_{KL}(Q \\| P)$.\n",
    "\n",
    "> * **Note**: If $P \\not\\ll Q$, we set $D_{KL}(P \\| Q) = +\\infty$.\n",
    "\n",
    "### `Properties`:\n",
    "\n",
    "1. **Non-negativity**: $D_{KL}(P \\| Q) \\geq 0$, with equality if and only if $P = Q$ almost everywhere.\n",
    "2. **Chain Rule**: if $P_{X,Y}$ and $Q_{X,Y}$ are joint distributions, then\n",
    "$$\n",
    "D_{KL}(P_{X,Y} \\| Q_{X,Y}) = D_{KL}(P_X \\| Q_X) + \\mathbb{E}_{P_X}[D_{KL}(P_{Y|X} \\| Q_{Y|X})]\n",
    "$$\n",
    "3. **Invariance under reparameterization**: KL divergence remains unchanged under smooth and invertible transformations of the random variables.\n",
    "\n",
    "\n",
    "### `Properties MUST be memorized`:\n",
    "\n",
    "1. Non-negativity;\n",
    "2. Additivity over independent samples;\n",
    "3. Locality (depends only on density ratios $\\frac{dP}{dQ}$).\n",
    "4. Continuity.\n",
    "5. Invariance under reparameterization.\n",
    "6. Compatibility with Bayesian Inference.\n",
    "7. Coding length stuff.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c9a7aa",
   "metadata": {},
   "source": [
    "## `Cross-Entropy and its relation to KL Divergence`:\n",
    "\n",
    "### **Related Concepts**:\n",
    "\n",
    "1. Entropy:\n",
    "$$\n",
    "H(P) := -\\int p(x). \\log{p(x)} dx\n",
    "$$\n",
    "\n",
    "2. Cross-Entropy:\n",
    "$$\n",
    "H(P, Q) := -\\int p(x). \\log{q(x)} dx\n",
    "$$\n",
    "\n",
    "3. KL Divergence:\n",
    "$$\n",
    "D(P\\|Q) := \\int p(x). \\log{\\frac{p(x)}{q(x)}} dx = H(P, Q) - H(P)\n",
    "$$\n",
    "\n",
    "> **Note**: Cross-Entropy - (uncertainty of the world) + (penalty for wrong model) = Entropy (uncertainty of the world).\n",
    "\n",
    "* $H(P)$ measures the uncertainty inherent of the data.\n",
    "* $D(P\\|Q)$ extra cost of encoding samples using the wrong distribution.\n",
    "\n",
    "The **Cross-Entropy** gives the expected number of bits needed if the world follows $P$, but we assumed $Q$.\n",
    "\n",
    "\n",
    "| **Quantity**       | **Formula**                              | **Interpretation**                                      |\n",
    "|--------------------|---------------------------------------------|---------------------------------------------------------|\n",
    "| Entropy            | $H(P) = -\\int p(x) \\log p(x) dx$ | Measures the uncertainty inherent in the distribution $P$. |\n",
    "| Cross-Entropy      | $H(P, Q) = -\\int p(x) \\log q(x) dx$ | Measures the cost using a model $Q$. |\n",
    "| Kullback-Leibler Divergence | $D(P \\| Q) = \\int p(x) \\log \\frac{p(x)}{q(x)} dx$ | Measures the extra cost due to mismatch |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9391bb",
   "metadata": {},
   "source": [
    "## `Mutual Information:`\n",
    "\n",
    "Picture the world as a dumpo of possible tiny outcomes. Probability model is just a rule that tells us how to weigh each tiny outcome.\n",
    "* Model A (True) might put most of the weight in a small region.\n",
    "* Model B (Our guess) might spread the weight differently.\n",
    "\n",
    "Comparing them boils down to finding local ratios:\n",
    "- At each possible outcome, how much more does A weigh it compared to B?\n",
    "This ratio is the most primitice geometric fact about measures.\n",
    "\n",
    "\n",
    "Now, say we observe a pair of things $(x, y)$. The pointwise information is simply the local surprise:\n",
    "\"How much more likely is this pair under the real joint behavior that it would be if the two where independent?\"\n",
    "\n",
    "- **Positive Value**: That pair happens more often together than if they were independent.\n",
    "- **Negative Value**: That pair happens less often together than if they were independent.\n",
    "- **Zero**: That pair happens just as often together as if they were independent.\n",
    "\n",
    "In another words, this is what we call **Pointwise Mutual Information (PMI)**, and can be treated as the local curvature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c6db87",
   "metadata": {},
   "source": [
    "### **`Def. 11.1` Pointwise Mutual Information (PMI)**:\n",
    "\n",
    "Given a pair of Random Variables $(X, Y)$, a measurable space with a joint probability distribution $P_{X,Y}$, and marginal distributions $P_X$ and $P_Y$, having $P_{X,Y} \\ll P_X \\otimes P_Y$, the Pointwise Mutual Information (PMI) between $X$ and $Y$ at the point $(x, y)$ is defined as:\n",
    "$$\n",
    "\\text{pmi}(x, y) := \\log\\left(\\frac{dP_{X,Y}}{d(P_X \\otimes P_Y)}(x, y)\\right)\n",
    "$$\n",
    "\n",
    "Whenever densities exist this reduces to:\n",
    "$$\n",
    "\\text{pmi}(x, y) = \\log\\left(\\frac{p(x, y)}{p(x) . p(y)}\\right)\n",
    "$$\n",
    "\n",
    "> * **Interpretation**: PMI shows us how surprising (or not) for us it is to observe the pair $(x, y)$ together, compared to what we would expect if $X$ and $Y$ were independent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cff7ff",
   "metadata": {},
   "source": [
    "### **`Def. 11.2` Mutual Information (MI)**:\n",
    "\n",
    "Given a pair of Random Variables $(X, Y)$, we take their expected Pointwise Mutual Information (PMI):\n",
    "$$\n",
    "\\text{MI}(X, Y) := E_{P_{XY}}[\\text{PMI}(X, Y)]\n",
    "$$\n",
    "\n",
    "Which is called **Mutual Information (MI)** between $X$ and $Y$.\n",
    "Equivalently this can be stated as:\n",
    "$$\n",
    "\\text{MI}(X, Y) = D_{KL}(P_{XY} \\| P_X \\otimes P_Y) = \\int \\log\\left(\\frac{dP_{XY}}{d(P_X \\otimes P_Y)}\\right) dP(X, Y)\n",
    "$$\n",
    "\n",
    "> * **Geometric Meaning**:\n",
    ">> 1. The joint $P_{XY}$ is a point in the probability measure space.\n",
    ">> 2. The product of marginals $P_X \\otimes P_Y$ is \"the closest independent distribution\".\n",
    ">> 3. The MI is the information-geometric distance between these two points, measured by KL divergence.\n",
    "\n",
    "So, MI tells how much non-independence there is between $X$ and $Y$.\n",
    "\n",
    "\n",
    "#### `Average Evidence`:\n",
    "- If the average is **ZERO**: On average, seeing $X$ gives no information about $Y$ (independent).\n",
    "- If the average is **LARGE**: On average, samples tends to carry evidence that the joint is not the product of marginals (dependent).\n",
    "\n",
    "\n",
    "### `Properties of Mutual Information`:\n",
    "\n",
    "1. Non-negativity: $\\text{MI}(X, Y) \\geq 0$, with equality if and only if $X$ and $Y$ are independent.\n",
    "2. Symmetry: $\\text{MI}(X, Y) = \\text{MI}(Y, X)$.\n",
    "3. Zero iff Independence: $\\text{MI}(X, Y) = 0 \\iff P_{X,Y} = P_X \\otimes P_Y$ ($X$ and $Y$ are independent).\n",
    "4. Chain Rule: $\\text{MI}(X, Y, Z) = \\text{MI}(X, Y) + \\text{MI}(X, Z | Y)$.\n",
    "5. Relation to Entropy: $\\text{MI}(X, Y) = H(X) + H(Y) - H(X, Y) = H(X) - H(X | Y) = H(Y) - H(Y | X)$.\n",
    "6. Data Processing Inequality: If $X \\to Y \\to Z$ forms a Markov chain, then $\\text{MI}(X, Z) \\leq \\text{MI}(X, Y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526eb7e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3076971",
   "metadata": {},
   "source": [
    "# `2. Gibbs’, Markov’s, and Chebyshev’s inequalities.`\n",
    "\n",
    "\n",
    "## **`Theorema 10.1` Gibbs’ Inequality**:\n",
    "\n",
    "Given a measurable space $(\\Omega, \\mathcal{F})$ and two probability measures $P$ and $Q$ on this space such that $P$ is absolutely continuous with respect to $Q$ (denoted as $P \\ll Q$), the Kullback-Leibler divergence from $Q$ to $P$ satisfies:\n",
    "$$\n",
    "D_{KL}(P \\| Q) \\geq 0\n",
    "$$\n",
    "with strict equality $\\iff$ $P = Q$ (so measere coincide).\n",
    "\n",
    "> * **Interpretation**: This inequality states that the KL divergence is always non-negative, and it is zero if and only if the two probability measures are identical almost everywhere.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b75faf1",
   "metadata": {},
   "source": [
    "## **`Theorema 10.2` Markov’s Inequality**:\n",
    "\n",
    "Given a random variable $X \\geq 0$ and a positive constant $a > 0$, the following inequality holds:\n",
    "$$\n",
    "P(X \\geq a) \\leq \\frac{E[X]}{a}\n",
    "$$\n",
    "\n",
    "> * **Interpretation**: This inequality provides an upper bound on the probability that a non-negative random variable exceeds a certain value, based on its expected value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bddbac",
   "metadata": {},
   "source": [
    "## **`Theorema 10.3` Chebyshev’s Inequality**:\n",
    "\n",
    "Given a random variable $X$ with finite expected value $\\mu = E[X]$ and finite variance $\\sigma^2 = Var(X)$, for any positive constant $k > 0$, we have a normalized Markov applied to $(X - E[X])^2$ , the following inequality holds:\n",
    "$$\n",
    "P(|X - \\mu| \\geq k) \\leq \\frac{\\sigma^2}{k^2}\n",
    "$$\n",
    "\n",
    "> * **Interpretation**: This inequality provides an upper bound on the probability that a random variable deviates from its mean by at least $k$ standard deviations, based on its variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d03b439",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b85b3b4",
   "metadata": {},
   "source": [
    "## `Context`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1471ac33",
   "metadata": {},
   "source": [
    "### **`Def.` Independent and Identically Distributed (i.i.d.) Random Variables**:\n",
    "\n",
    "Better known as ***i.i.d.***, a sequence of random variables $\\{X_1, X_2, \\ldots, X_n\\}$ is a concept meaning that each data point (or random variable) in a dataset is unrelated to the others (independent) and all data points are drawn from the same probability distribution (identically distributed).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c57020a",
   "metadata": {},
   "source": [
    "### **`Def.` Mean**:\n",
    "\n",
    "Mean is a measure of central tendency that represents the \"average\" or \"center\" value of a entire set of numbers or probability distribution. It is calculated by summing all the values in the dataset and then dividing by the total number of values:\n",
    "$$\n",
    "\\mu = \\frac{1}{n} \\sum_{i=1}^{n} x_i = \\frac{x_1 + \\cdots + x_n}{n} = \\overline{X}, \\quad \\text{ where } x_i \\in X \\quad \\forall i=1,\\ldots,n\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bdd291",
   "metadata": {},
   "source": [
    "In the context of probability distribution, $\\mu$ is synonymous with the expected value $E[X]$ of a random variable $X$.\n",
    "\n",
    "* **Discrete Random Variable**:\n",
    "  $$\n",
    "  E[X] = \\sum_{i}[x . P(x)] = \\sum_{i} x_i . p_X(x_i)\n",
    "  $$\n",
    "\n",
    "* **Continuous Random Variable**:\n",
    "  $$\n",
    "  E[X] = \\int_{-\\infty}^{\\infty} x . f(x) dx\n",
    "  $$\n",
    "  where $f(x)$ is the probability density function (PDF) of the random variable $X$.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83efa36",
   "metadata": {},
   "source": [
    "### **`Def.` Variance**:\n",
    "\n",
    "Variance is measure of dispersion that quantifies how far a set of numbers is spread out from their average (mean) value.\n",
    "\n",
    "1. **Low Variance**: Data points are clustered closely around the mean, indicating consistency.\n",
    "2. **High Variance**: Data points are spread out over a wider range, indicating greater variability.\n",
    "3. **Zero Variance**: All data points are identical, indicating no variability.\n",
    "\n",
    "The mathematical formulation depends on whether we are dealing with a population or a sample:\n",
    "\n",
    "| **Type**       | **Symbol**  | **Formula**   | **Explanation**                              |\n",
    "|----------------|-------------|---------------|----------------------------------------------|\n",
    "| Population     | $\\sigma^2$  | $\\sigma^2 = \\frac{\\sum_{i=1}^{N} (x_i - \\mu)^2}{N}$ | Variance of the entire population. Divided by the total number of observations (N)          |\n",
    "| Sample         | $s^2$        | $s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\overline{x})^2}{n - 1}$ | Variance of a sample from the population. Divided by $n-1$ (**Basel's correctio**) to provide an unbiased estimate of population varianve   |\n",
    "\n",
    "<br>\n",
    "\n",
    "> **Note**: $\\mu$ is the population mean, and $\\overline{x}$ is the sample mean.\n",
    "\n",
    "For a **Random Variable** $X$, the variance is defined as the **expected value of the squared deviation from the mean** $\\mu$:\n",
    "$$\n",
    "Var(X) = E[(X - \\mu)^2] = E[X^2] - (E[X])^2\n",
    "$$\n",
    "\n",
    "**`Properties of Variance`**:\n",
    "\n",
    "1. **Non-Negativity**: Variance is always non-negative, i.e., $Var(X) \\geq 0$.\n",
    "2. **Squared Units**: Variance is expressed in squared units of the original data.(e.g., if data is in meters, variance is in square meters).\n",
    "3. **Relationship to Standard Deviation**: The standard deviation ($\\sigma$) is the square root of the variance, providing a measure of spread in the same units as the data.\n",
    "4. **Sensitivity to Outliers**: Variance is sensitive to extreme values (outliers) because it squares the deviations from the mean.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a918d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18e0fb2d",
   "metadata": {},
   "source": [
    "## `4. Weak and strong laws of large numbers`\n",
    "\n",
    "\n",
    "### **`Def.` Law of Large Numbers**:\n",
    "\n",
    "Assume we have a sequence of independent and identically distributed (i.i.d.) random variables $\\{X_1, X_2, \\ldots, X_n\\}$, with finite mean $\\mu$ and a finite variance $\\sigma^2$.\n",
    "Let $\\overline{X}_n = \\frac{x_1 + \\ldots + x_n}{n}$ be the sample mean (of $X_1$ through $X_n$) - it itself is a random variable with mean $\\mu$:\n",
    "$$\n",
    "E[\\overline{X}_n] = \\frac{E(X_1 + X_2 + \\ldots + X_n)}{n} = \\frac{E(X_1) + \\ldots + E(X_n)}{n} = \\mu\n",
    "$$\n",
    "\n",
    "and variance $\\frac{\\sigma^2}{n}$:\n",
    "$$\n",
    "Var(\\overline{X}_n) = Var\\left(\\frac{X_1 + X_2 + \\ldots + X_n}{n}\\right) = \\frac{Var(X_1) + \\ldots + Var(X_n)}{n^2} = \\frac{n \\sigma^2}{n^2} = \\frac{\\sigma^2}{n}\n",
    "$$\n",
    "\n",
    "> * **Note**: The ***Law of Large Numbers (LLN)*** states that as $n$ grows larger, the sample mean $\\overline{X}_n$ converges to the true mean $\\mu$. **LLN** has two main forms: the **Weak Law of Large Numbers (WLLN)** and the **Strong Law of Large Numbers (SLLN)**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17904ad4",
   "metadata": {},
   "source": [
    "### **`Theorem 10.2.1` Strong Law of Large Numbers (SLLN)**:\n",
    "\n",
    "The sample mean $\\overline{X}_n$ converges to the true mean $\\mu$ pointwise, with probability 1, as the number of observations $n$ approaches infinity:\n",
    "$$\n",
    "P\\left(\\lim_{n \\to \\infty} \\overline{X}_n = \\mu\\right) = 1\n",
    "$$\n",
    "\n",
    "> * Recall that random variables are functions from the sample space $\\Omega \\in \\mathbb{R}$ - **pointwise convergence** says that $\\overline{X_n} \\to \\mu$ for each point $\\omega \\in \\Omega$, except maybe some set $B_0$ with $P(B_0) = 0$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecadfefb",
   "metadata": {},
   "source": [
    "### **`Theorem 10.2.2` Weak Law of Large Numbers (WLLN)**:\n",
    "\n",
    "For all $\\epsilon > 0$, $P(|\\overline{X}_n - \\mu| \\geq \\epsilon) \\to 0$ as $n \\to \\infty$. (This is convergence in probability).\n",
    "\n",
    "In other words, for any small positive number $\\epsilon$, the probability that the sample mean $\\overline{X}_n$ deviates from the true mean $\\mu$ by at least $\\epsilon$ approaches zero as the number of observations $n$ increases indefinitely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568ffde6",
   "metadata": {},
   "source": [
    "> * **Interpretation**: The LLN is essential for simulations, statistics and science in general. When generating data by replicating an experiment and averaging the result to approximate the theoretical avearge, we appeal to LLN.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3049a68e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8a356f5",
   "metadata": {},
   "source": [
    "## `5. Central limit theorem`\n",
    "\n",
    "Let's assume: \n",
    "1. We have a sequence of independent and identically distributed (i.i.d.) random variables $\\{X_1, X_2, \\ldots, X_n\\}$;\n",
    "2. Each with finite **mean $\\mu$** and finite **variance $\\sigma^2$**. \n",
    "3. $\\overline{X}_n = \\frac{X_1 + X_2 + \\ldots + X_n}{n}$ be the sample mean of these random variables.\n",
    "\n",
    "Then, LLN says that as $n \\to \\infty$, the sample mean $\\overline{X}_n$ converges to the true mean $\\mu$.\n",
    "\n",
    "What about its distribution?\n",
    "\n",
    "### **`Theorem 10.3.1` Central Limit Theorem (CLT)**:\n",
    "\n",
    "As $n \\to \\infty$, the distribution of the standardized sample mean approaches a standard normal distribution $N(0, 1)$:\n",
    "\n",
    "$$\n",
    "Z_n = \\sqrt{n} . \\left(\\frac{\\overline{X}_n - \\mu}{\\sigma}\\right) \\xrightarrow{} N(0, 1)\n",
    "$$\n",
    "\n",
    "> * **Interpretation**: The CLT states that regardless of the original distribution of the data, the distribution of the sample mean will tend to be normal as the sample size increases. This is crucial for inferential statistics, as it justifies the use of normal distribution-based methods for hypothesis testing and confidence intervals, even when the underlying data is not normally distributed. \n",
    "\n",
    "In anoter words, that means that the CDF of the left-hand side (l.h.s.) converges to the CDF of the right-hand side (r.h.s.):\n",
    "$$\n",
    "\\lim_{n \\to \\infty} P\\left(Z_n \\leq z\\right) = \\Phi(z)\n",
    "$$\n",
    "where $\\Phi(z)$ is the CDF of the standard normal distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2c43a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
