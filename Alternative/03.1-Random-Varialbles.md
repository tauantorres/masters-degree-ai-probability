# `03.1. Random Variables & Elements`

1. Random variables and random elements: motivation and examples.
2. Transformations of random variables.
3. Distribution of a random variable.
4. Characteristic function and moment-generating function.
5. Moments and cumulants.
6. Families of distributions. 
7. Statistics. 
8. Entropy. 
9. Independent random variables, marginals.

---

### **`Topic 3.1` Random Variables & Random Elements**:

**Motivation**: We need a way to map abstract experimental outcomes (like "Heads" or "Success") into a numerical space so we can perform calculations.

*   **Random Variable ($X$)**: A measurable function $X: \Omega \to \mathbb{R}$ that maps the sample space to real numbers.
*   **Random Element**: A generalization where the mapping goes to more complex spaces (e.g., vectors, functions, or graphs). 
*   **Example**: If $\Omega$ is "Weather", $X$ might map "Sunny" $\to 1$ and "Rainy" $\to 0$.

---

### **`Topic 3.2` Transformations of Random Variables**:

If we have a random variable $X$ and a function $g(x)$, we often need to find the distribution of $Y = g(X)$.

*   **Discrete**: $P(Y=y) = \sum_{x: g(x)=y} P(X=x)$.
*   **Continuous (Monotonic)**: If $g$ is differentiable and strictly increasing/decreasing, the PDF of $Y$ is:
    $$f_Y(y) = f_X(g^{-1}(y)) \left| \frac{d}{dy} g^{-1}(y) \right|$$
*   **Key Tool**: The **Method of CDFs** is often safer: Find $F_Y(y) = P(g(X) \le y)$ and then differentiate to get $f_Y(y)$.

---

### **`Topic 3.3` Distribution of a Random Variable**:

The **Distribution** is the "blueprint" of a random variable. It describes the probability assigned to every measurable set.
*   **Law ($P_X$)**: The pushforward measure $P_X(B) = P(X^{-1}(B))$.
*   **Representations**: A distribution is fully described by its **CDF** ($F_X$), its **PDF/PMF**, or its **Characteristic Function**.

---

### **`Topic 3.4` Characteristic Function (CF) & Moment-Generating Function (MGF)**:

These are "transform" methods that encode all the information about a distribution into a function.

| Feature | Moment-Generating Function (MGF) | Characteristic Function (CF) |
| :--- | :--- | :--- |
| **Formula** | $M_X(t) = E[e^{tX}]$ | $\phi_X(t) = E[e^{itX}]$ |
| **Existence** | May not exist for all $t$ (e.g., Cauchy). | **Always exists** for any RV. |
| **Use Case** | Calculating moments easily. | Theoretical proofs (e.g., CLT). |

---

### **`Topic 3.5` Moments and Cumulants**:

These describe the "features" of the distribution shape.

*   **Raw Moments**: $\mu'_n = E[X^n]$. (1st raw moment = Mean).
*   **Central Moments**: $\mu_n = E[(X - \mu)^n]$. (2nd central moment = Variance).
*   **Cumulants ($\kappa_n$)**: Derived from the **Cumulant Generating Function** $K_X(t) = \ln(M_X(t))$.
    *   $\kappa_1$ = Mean.
    *   $\kappa_2$ = Variance.
    *   $\kappa_3$ = 3rd central moment (related to Skewness).

---

### **`Topic 3.6` Families of Distributions**:

Distributions are categorized based on their properties and parameters.

*   **Location-Scale Families**: Distributions related by shifting ($x+\mu$) or scaling ($\sigma x$). Example: The Normal distribution $N(\mu, \sigma^2)$.
*   **Exponential Families**: A class of distributions (Normal, Binomial, Poisson) whose PDF can be written in a specific exponential form. They have "sufficient statistics," making them vital for estimation theory.

---

### **`Topic 3.7` Statistics**:

A **Statistic** is a function of the sample data $T(X_1, \dots, X_n)$ that does not depend on any unknown parameters.
*   **Example**: The sample mean $\bar{X} = \frac{1}{n} \sum X_i$.
*   **Purpose**: We use statistics to make inferences about unknown population **Parameters** (like $\mu$ or $\sigma$).

---

### **`Topic 3.8` Entropy ($H$)**:

**Entropy** measures the average amount of "surprise" or uncertainty in a distribution.

*   **Formula (Discrete)**: $H(X) = -\sum P(x) \log P(x)$.
*   **Uniform Distribution**: Has the **highest entropy** for a finite set (maximum uncertainty).
*   **Delta Distribution**: Has **zero entropy** (the outcome is certain).

---

### **`Topic 3.9` Independent Random Variables & Marginals**:

When dealing with multiple variables $(X, Y)$:

*   **Independence**: $X$ and $Y$ are independent if $f_{X,Y}(x,y) = f_X(x) f_Y(y)$. Knowledge of $X$ tells you nothing about $Y$.
*   **Marginals**: If you have the joint PDF $f_{X,Y}$, the marginal $f_X(x)$ is found by "summing out" $Y$:
    $$f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x, y) dy$$
