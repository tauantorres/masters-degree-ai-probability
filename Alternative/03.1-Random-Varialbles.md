# `03.1. Random Variables & Elements`

1. Random variables and random elements: motivation and examples.
2. Transformations of random variables.
3. Distribution of a random variable.
4. Characteristic function and moment-generating function.
5. Moments and cumulants.
6. Families of distributions. 
7. Statistics. 
8. Entropy. 
9. Independent random variables, marginals.

---

### **`Topic 3.1` Random Variables & Random Elements**:

**Motivation**: We need a way to map abstract experimental outcomes (like "Heads" or "Success") into a numerical space so we can perform calculations.

*   **Random Variable ($X$)**: A measurable function $X: \Omega \to \mathbb{R}$ that maps the sample space to real numbers.
*   **Random Element**: A generalization where the mapping goes to more complex spaces (e.g., vectors, functions, or graphs). 
*   **Example**: If $\Omega$ is "Weather", $X$ might map "Sunny" $\to 1$ and "Rainy" $\to 0$.

### **`Def. 3.1.1` Random Variable (The Source)**:

A **Random Variable (RV)** is a numerical description of the outcome of a statistical experiment. It maps outcomes from a sample space to real numbers.
*   **Discrete RV**: Values you can count (e.g., $0, 1, 2$).
*   **Continuous RV**: Values in a range (e.g., $1.75\dots$ meters).

---

### **`Topic 3.2` Transformations of Random Variables**:

If we have a random variable $X$ and a function $g(x)$, we often need to find the distribution of $Y = g(X)$.

*   **Discrete**: $P(Y=y) = \sum_{x: g(x)=y} P(X=x)$.
*   **Continuous (Monotonic)**: If $g$ is differentiable and strictly increasing/decreasing, the PDF of $Y$ is:
    $$f_Y(y) = f_X(g^{-1}(y)) \left| \frac{d}{dy} g^{-1}(y) \right|$$
*   **Key Tool**: The **Method of CDFs** is often safer: Find $F_Y(y) = P(g(X) \le y)$ and then differentiate to get $f_Y(y)$.

---

### **`Topic 3.3` Distribution of a Random Variable**:

The **Distribution** is the "blueprint" of a random variable. It describes the probability assigned to every measurable set.
*   **Law ($P_X$)**: The pushforward measure $P_X(B) = P(X^{-1}(B))$.
*   **Representations**: A distribution is fully described by its **CDF** ($F_X$), its **PDF/PMF**, or its **Characteristic Function**.

### **`Def. 3.3.1` Probability Mass Function (PMF) - [Discrete Only]**:

For discrete random variables, the **PMF** gives the probability that the variable is exactly equal to some value.

$$p_X(x) = P(X = x)$$

**Key Rules**:
1. All probabilities are between 0 and 1.
2. The sum of all probabilities must equal 1: $\sum p_X(x) = 1$.


### **`Def. 3.3.2` Probability Density Function (PDF) - [Continuous Only]**:

For continuous random variables, the probability of hitting a *single exact point* is zero ($P(X=1.5) = 0$). Instead, we use the **PDF** to find probability over an **interval**.

$$P(a \le X \le b) = \int_{a}^{b} f_X(x) dx$$

**Key Rules**:
1. $f_X(x) \ge 0$ (Density cannot be negative).
2. The total area under the curve must be 1: $\int_{-\infty}^{\infty} f_X(x) dx = 1$.

### **`Def. 3.3.3` Cumulative Distribution Function (CDF) - [Universal]**:

The **CDF** is the most powerful tool because it applies to *both* discrete and continuous variables. It calculates the "running total" of probability up to a point $x$.

$$F_X(x) = P(X \le x)$$

**The Relationship "The Calculus of Stats"**:
*   **To get CDF from PDF**: Integrate. $F_X(x) = \int_{-\infty}^{x} f_X(t) dt$.
*   **To get PDF from CDF**: Differentiate. $f_X(x) = \frac{d}{dx} F_X(x)$.

### **`Def. 3.3.4` Joint, Marginal, and Conditional Distributions**:

When studying two variables ($X$ and $Y$) simultaneously:

1.  **Joint Distribution**: The probability that $X$ and $Y$ happen at the same time.
    *   $f_{X,Y}(x,y)$
2.  **Marginal Distribution**: The distribution of just $X$, ignoring $Y$.
    *   Find it by "collapsing" $Y$: $f_X(x) = \int f_{X,Y}(x,y) dy$.
3.  **Conditional Distribution**: The distribution of $X$ *given* that we know $Y$ has already happened.
    *   $f_{X|Y}(x|y) = \frac{f_{X,Y}(x,y)}{f_Y(y)}$

---

### **Summary for Memorization**:

| Term | Symbol | Question it Answers | Data Type |
| :--- | :--- | :--- | :--- |
| **PMF** | $p(x)$ | "What is the chance of exactly $x$?" | Discrete |
| **PDF** | $f(x)$ | "How dense is the probability near $x$?" | Continuous |
| **CDF** | $F(x)$ | "What is the chance of getting $x$ or less?" | Both |
| **Joint** | $f(x,y)$ | "What is the chance of $x$ AND $y$?" | Both |

---

### **`Topic 3.4` Characteristic Function (CF) & Moment-Generating Function (MGF)**:

These are "transform" methods that encode all the information about a distribution into a function.

| Feature | Moment-Generating Function (MGF) | Characteristic Function (CF) |
| :--- | :--- | :--- |
| **Formula** | $M_X(t) = E[e^{tX}]$ | $\phi_X(t) = E[e^{itX}]$ |
| **Existence** | May not exist for all $t$ (e.g., Cauchy). | **Always exists** for any RV. |
| **Use Case** | Calculating moments easily. | Theoretical proofs (e.g., CLT). |

---

### **`Topic 3.5` Moments and Cumulants**:

These describe the "features" of the distribution shape.

*   **Raw Moments**: $\mu'_n = E[X^n]$. (1st raw moment = Mean).
*   **Central Moments**: $\mu_n = E[(X - \mu)^n]$. (2nd central moment = Variance).
*   **Cumulants ($\kappa_n$)**: Derived from the **Cumulant Generating Function** $K_X(t) = \ln(M_X(t))$.
    *   $\kappa_1$ = Mean.
    *   $\kappa_2$ = Variance.
    *   $\kappa_3$ = 3rd central moment (related to Skewness).

---

### **`Topic 3.6` Families of Distributions**:

Distributions are categorized based on their properties and parameters.

*   **Location-Scale Families**: Distributions related by shifting ($x+\mu$) or scaling ($\sigma x$). Example: The Normal distribution $N(\mu, \sigma^2)$.
*   **Exponential Families**: A class of distributions (Normal, Binomial, Poisson) whose PDF can be written in a specific exponential form. They have "sufficient statistics," making them vital for estimation theory.

---

### **`Topic 3.7` Statistics**:

A **Statistic** is a function of the sample data $T(X_1, \dots, X_n)$ that does not depend on any unknown parameters.
*   **Example**: The sample mean $\bar{X} = \frac{1}{n} \sum X_i$.
*   **Purpose**: We use statistics to make inferences about unknown population **Parameters** (like $\mu$ or $\sigma$).

---

### **`Topic 3.8` Entropy ($H$)**:

**Entropy** measures the average amount of "surprise" or uncertainty in a distribution.

*   **Formula (Discrete)**: $H(X) = -\sum P(x) \log P(x)$.
*   **Uniform Distribution**: Has the **highest entropy** for a finite set (maximum uncertainty).
*   **Delta Distribution**: Has **zero entropy** (the outcome is certain).

---

### **`Topic 3.9` Independent Random Variables & Marginals**:

When dealing with multiple variables $(X, Y)$:

*   **Independence**: $X$ and $Y$ are independent if $f_{X,Y}(x,y) = f_X(x) f_Y(y)$. Knowledge of $X$ tells you nothing about $Y$.
*   **Marginals**: If you have the joint PDF $f_{X,Y}$, the marginal $f_X(x)$ is found by "summing out" $Y$:
    $$f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x, y) dy$$

### **`Def. 3.9.1` Independence of Random Variables**:

Two random variables $X$ and $Y$ are **independent** if for all measurable sets $A, B$, the events $\{X \in A\}$ and $\{Y \in B\}$ are independent. This implies:

$$P(X \in A, Y \in B) = P(X \in A)P(Y \in B)$$

**Properties under Independence**:
1. $E[XY] = E[X]E[Y]$
2. $Cov(X, Y) = 0$
3. $Var(X + Y) = Var(X) + Var(Y)$

### **`Def. 3.9.2` Marginal Distributions**:

Given a joint distribution (PDF $f_{X,Y}$ or PMF $p_{X,Y}$) of two random variables $X$ and $Y$, the **Marginal Distribution** of one variable is obtained by "summing out" or "integrating out" the other variable.

1. **Marginal PMF (Discrete)**: 
$$p_X(x) = \sum_{y \in S_y} p_{X,Y}(x, y)$$
2. **Marginal PDF (Continuous)**: 
$$f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x, y) dy$$

> **Note**: The marginal distribution provides the probability behavior of $X$ ignoring all information about $Y$.
