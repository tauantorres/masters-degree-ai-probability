# `03.2. Random Variables`

1. Random variables and random elements: motivation and examples.
2. Transformations of random variables.
3. Distribution of a random variable.
4. Characteristic function and moment-generating function.
5. Moments and cumulants.
6. Families of distributions. 
7. Statistics. 
8. Entropy. 
9. Independent random variables, marginals.


---

### **`Def. 2.7` Expected Value ($E$ / Mean $\mu$ )**:

Given a probability space $(\Omega,\mathcal{F},P)$ and a random variable $X$, the **Expected Value** (or mean) is the weighted average of all possible values of $X$. It is defined as:

1. **Discrete Case**: 
$$E[X] = \sum_{i} x_i P(X = x_i)$$
2. **Continuous Case**: 
$$E[X] = \int_{-\infty}^{\infty} x f_X(x) dx$$

**Properties**:
* **Linearity**: $E[aX + bY] = aE[X] + bE[Y]$ for constants $a, b$.
* **Expectation of a function ($X \to g(X)$)**: $E[g(X)] = \int g(x) f_X(x) dx$.

Where $f_X(x)$ is the probability density function of $X$.

---

### **`Def. 2.8` Variance ($\sigma^2$)**:

The **Variance** of a random variable $X$ measures the spread of the distribution around its mean $\mu = E[X]$. It is defined as:

$$Var(X) = E[(X - E[X])^2]$$

**Properties**:
1. **Alternative Formula**: $Var(X) = E[X^2] - (E[X])^2$
2. **Non-negativity**: $Var(X) \ge 0$
3. **Scaling**: $Var(aX + b) = a^2 Var(X)$

---

### **`Def. 2.9` Covariance ($Cov$)**:

Given two random variables $X$ and $Y$ defined on the same probability space, the **Covariance** measures their joint linear variability:

$$Cov(X, Y) = E[(X - E[X])(Y - E[Y])]$$

**Properties**:
1. **Alternative Formula**: $Cov(X, Y) = E[XY] - E[X]E[Y]$
2. **Symmetry**: $Cov(X, Y) = Cov(Y, X)$
3. **Relation to Variance**: $Cov(X, X) = Var(X)$
4. **Variance of a Sum**: $Var(X + Y) = Var(X) + Var(Y) + 2Cov(X, Y)$

---

### **`Def. 2.10` Independence of Random Variables**:

Two random variables $X$ and $Y$ are **independent** if for all measurable sets $A, B$, the events $\{X \in A\}$ and $\{Y \in B\}$ are independent. This implies:

$$P(X \in A, Y \in B) = P(X \in A)P(Y \in B)$$

**Properties under Independence**:
1. $E[XY] = E[X]E[Y]$
2. $Cov(X, Y) = 0$
3. $Var(X + Y) = Var(X) + Var(Y)$

---

### **`Def. 2.11` Marginal Distributions**:

Given a joint distribution (PDF $f_{X,Y}$ or PMF $p_{X,Y}$) of two random variables $X$ and $Y$, the **Marginal Distribution** of one variable is obtained by "summing out" or "integrating out" the other variable.

1. **Marginal PMF (Discrete)**: 
$$p_X(x) = \sum_{y \in S_y} p_{X,Y}(x, y)$$
2. **Marginal PDF (Continuous)**: 
$$f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x, y) dy$$

> **Note**: The marginal distribution provides the probability behavior of $X$ ignoring all information about $Y$.


---

### **`Def. 2.12` Random Variable (The Source)**:

A **Random Variable (RV)** is a numerical description of the outcome of a statistical experiment. It maps outcomes from a sample space to real numbers.
*   **Discrete RV**: Values you can count (e.g., $0, 1, 2$).
*   **Continuous RV**: Values in a range (e.g., $1.75\dots$ meters).

---

### **`Def. 2.13` Probability Mass Function (PMF) - [Discrete Only]**:

For discrete random variables, the **PMF** gives the probability that the variable is exactly equal to some value.

$$p_X(x) = P(X = x)$$

**Key Rules**:
1. All probabilities are between 0 and 1.
2. The sum of all probabilities must equal 1: $\sum p_X(x) = 1$.

---

### **`Def. 2.14` Probability Density Function (PDF) - [Continuous Only]**:

For continuous random variables, the probability of hitting a *single exact point* is zero ($P(X=1.5) = 0$). Instead, we use the **PDF** to find probability over an **interval**.

$$P(a \le X \le b) = \int_{a}^{b} f_X(x) dx$$

**Key Rules**:
1. $f_X(x) \ge 0$ (Density cannot be negative).
2. The total area under the curve must be 1: $\int_{-\infty}^{\infty} f_X(x) dx = 1$.

---

### **`Def. 2.15` Cumulative Distribution Function (CDF) - [Universal]**:

The **CDF** is the most powerful tool because it applies to *both* discrete and continuous variables. It calculates the "running total" of probability up to a point $x$.

$$F_X(x) = P(X \le x)$$

**The Relationship "The Calculus of Stats"**:
*   **To get CDF from PDF**: Integrate. $F_X(x) = \int_{-\infty}^{x} f_X(t) dt$.
*   **To get PDF from CDF**: Differentiate. $f_X(x) = \frac{d}{dx} F_X(x)$.

---

### **`Def. 2.16` Joint, Marginal, and Conditional Distributions**:

When studying two variables ($X$ and $Y$) simultaneously:

1.  **Joint Distribution**: The probability that $X$ and $Y$ happen at the same time.
    *   $f_{X,Y}(x,y)$
2.  **Marginal Distribution**: The distribution of just $X$, ignoring $Y$.
    *   Find it by "collapsing" $Y$: $f_X(x) = \int f_{X,Y}(x,y) dy$.
3.  **Conditional Distribution**: The distribution of $X$ *given* that we know $Y$ has already happened.
    *   $f_{X|Y}(x|y) = \frac{f_{X,Y}(x,y)}{f_Y(y)}$

---

### **Summary for Memorization**:

| Term | Symbol | Question it Answers | Data Type |
| :--- | :--- | :--- | :--- |
| **PMF** | $p(x)$ | "What is the chance of exactly $x$?" | Discrete |
| **PDF** | $f(x)$ | "How dense is the probability near $x$?" | Continuous |
| **CDF** | $F(x)$ | "What is the chance of getting $x$ or less?" | Both |
| **Joint** | $f(x,y)$ | "What is the chance of $x$ AND $y$?" | Both |


---

### **`Def. 2.17` Moments of a Random Variable**:

Moments are a set of statistical measures used to describe the shape of a distribution.
1.  **1st Moment**: $E[X]$ (Mean) - Location of the center.
2.  **2nd Central Moment**: $E[(X-\mu)^2]$ (Variance) - Width of the spread.
3.  **3rd Central Moment**: **Skewness** - Measures asymmetry (leaning left or right).
4.  **4th Central Moment**: **Kurtosis** - Measures "tailedness" (how many outliers exist).

---

### **`Def. 2.18` Law of the Unconscious Statistician (LOTUS)**:

A critical theorem used to calculate the expected value of a **function** of a random variable without needing to find the distribution of $g(X)$ first:

$$E[g(X)] = \int_{-\infty}^{\infty} g(x) f_X(x) dx \quad \text{(Continuous)}$$
$$E[g(X)] = \sum_{x} g(x) p_X(x) \quad \text{(Discrete)}$$

---

### **`Def. 2.19` Moment Generating Function (MGF)**:

The **MGF** is a functional "DNA" of a random variable. If two variables have the same MGF, they have the same distribution. It is defined as:

$$M_X(t) = E[e^{tX}]$$

**Why it matters**: 
To find the $n$-th moment, you simply take the $n$-th derivative of $M_X(t)$ and evaluate it at $t=0$:
$$E[X^n] = M_X^{(n)}(0)$$

---

### **`Def. 2.20` Correlation ($\rho$)**:

While **Covariance** tells you the direction of a relationship, it is scale-dependent. **Correlation** is the "standardized" version that scales the relationship between $-1$ and $+1$.

$$\rho_{X,Y} = \text{Corr}(X,Y) = \frac{Cov(X,Y)}{\sigma_X \sigma_Y}$$

*   **$\rho = 1$**: Perfect positive linear relationship.
*   **$\rho = -1$**: Perfect negative linear relationship.
*   **$\rho = 0$**: No linear relationship.

---

### **`Def. 2.21` Standard Deviation ($\sigma$)**:

The **Standard Deviation** is simply the square root of the variance. We use it because it returns the "spread" measurement back to the **original units** of the data.

$$\sigma = \sqrt{Var(X)}$$

---

### **Summary of Important Inequalities**:

1.  **Markov's Inequality**: Provides an upper bound for the probability that a non-negative RV is greater than a constant.
    $$P(X \ge a) \le \frac{E[X]}{a}$$
2.  **Chebyshev's Inequality**: Guarantees that nearly all values are close to the mean (used for outlier detection).
    $$P(|X - \mu| \ge k\sigma) \le \frac{1}{k^2}$$


# `Third Lector`: Advanced Topics in Random Variables

---

### **`Topic 3.1` Families of Distributions**:

**Families of Distributions** are standardized probability models that describe common real-world phenomena. They have pre-defined parameters that determine their shape, center, and spread.

#### A. Discrete Families

| Name | Notation | Parameters | Common Use |
| :--- | :--- | :--- | :--- |
| **Bernoulli** | $Bern(p)$ | $p \in [0, 1]$ | Single coin flip outcome (Success/Failure). |
| **Binomial** | $Bin(n, p)$ | $n \in \mathbb{N}, p \in [0, 1]$ | Count of successes in $n$ independent trials. |
| **Poisson** | $Pois(\lambda)$ | $\lambda > 0$ | Count of rare events over a fixed time/space interval (e.g., calls per hour). |

#### B. Continuous Families

| Name | Notation | Parameters | Common Use |
| :--- | :--- | :--- | :--- |
| **Uniform** | $U(a, b)$ | $a, b \in \mathbb{R}$ | Events equally likely to occur within an interval. |
| **Exponential** | $Exp(\lambda)$ | $\lambda > 0$ | Time until the next event occurs (memoryless property). |
| **Normal (Gaussian)** | $N(\mu, \sigma^2)$ | $\mu \in \mathbb{R}, \sigma^2 > 0$ | Central Limit Theorem; most common natural distribution (heights, errors). |

---

### **`Topic 3.2` Statistics**:

In this context, a **Statistic** is any function of the observable data (a sample) that does not depend on unknown population parameters. It is itself a random variable.

| Term | Definition | Example |
| :--- | :--- | :--- |
| **Statistic** | A quantity calculated from sample data used to estimate a population parameter. | Sample Mean ($\bar{x}$), Sample Variance ($s^2$). |
| **Parameter** | A fixed, unknown value describing the entire population. | Population Mean ($\mu$), Population Variance ($\sigma^2$). |
| **Estimator** | A specific type of statistic used to estimate a parameter. | $\bar{x}$ is an estimator for $\mu$. |
| **Bias** | The difference between the expected value of an estimator and the true parameter value: $Bias(\hat{\theta}) = E[\hat{\theta}] - \theta$. |
| **Efficiency** | The inverse of the variance of an estimator (lower variance is better). |

---

### **`Topic 3.3` Entropy ($H$)**:

**Entropy** in information theory measures the average level of "information," "surprise," or uncertainty inherent in a random variable's possible outcomes.

*   **Higher Entropy**: More uncertainty/randomness (e.g., flipping a fair coin).
*   **Lower Entropy**: Less uncertainty/more predictability (e.g., a coin weighted to land on heads 99% of the time).

#### Definitions:
*   **Discrete Entropy (Shannon Entropy)**: 
    $$H(X) = -E[\log P(X)] = -\sum_{x} P(x) \log P(x)$$
*   **Differential Entropy (Continuous)**:
    $$H(X) = -\int_{-\infty}^{\infty} f_X(x) \log f_X(x) dx$$

---

### **`Topic 3.4` Independent Random Variables, Marginals**:

These topics link back to concepts covered in the previous Lector notes but emphasize their role in multivariate analysis:

#### A. Independence Review
$X$ and $Y$ are independent ($X \perp Y$) if their joint probability factors into the product of their individual (marginal) probabilities:
$$P(X \in A, Y \in B) = P(X \in A)P(Y \in B)$$

#### B. Marginal Distributions Review
The marginal distribution of one variable is derived from a joint distribution by integrating or summing over all possible values of the other variable:

$$f_X(x) = \int f_{X,Y}(x, y) dy$$

**Key Takeaway**: If you know $X$ and $Y$ are independent, finding their marginals is trivial because $f_{X,Y}(x,y) = f_X(x) \cdot f_Y(y)$.

---

### **`Topic 3.5` Characteristic Function (CF) and Moment-Generating Function (MGF)**:

These are alternative ways to characterize a distribution fully. The **Characteristic Function ($\phi_X(t)$)** is often preferred theoretically because it *always* exists, unlike the MGF.

*   **Moment-Generating Function (MGF)**:
    $$M_X(t) = E[e^{tX}]$$
    Used to find moments: $E[X^n] = M_X^{(n)}(0)$.
*   **Characteristic Function (CF)**:
    $$\phi_X(t) = E[e^{itX}]$$
    Involves imaginary numbers ($i^2 = -1$). It uniquely determines the distribution and is fundamental for proofs using the Central Limit Theorem.
