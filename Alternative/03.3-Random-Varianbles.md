# `03. Random Variables`

1. Random variables and random elements: motivation and examples.
2. Transformations of random variables.
3. Distribution of a random variable.
4. Characteristic function and moment-generating function.
5. Moments and cumulants.
6. Families of distributions. 
7. Statistics. 
8. Entropy. 
9. Independent random variables, marginals.

---

### **`Topic 3.1` Random Variables & Random Elements**

**Motivation**: We need a mathematical bridge to map abstract experimental outcomes (like "Heads" or "Success") into a numerical space to perform analysis.

**`Def. 3.1.1` Random Variable (The Source)**:
A **Random Variable (RV)** is a measurable function $X: \Omega \to \mathbb{R}$ that maps each outcome in the sample space to a real number.
*   **Discrete RV**: Takes on values in a countable set (e.g., $\{0, 1, 2, \dots\}$).
*   **Continuous RV**: Takes on values in an interval (e.g., $x \in [0, 1]$).

**`Def. 3.1.2` Random Element**:
A generalization where the mapping goes to more complex measurable spaces $(Y, \mathcal{Y})$, such as vectors, functions, or random graphs.

---

### **`Topic 3.2` Transformations of Random Variables**

When we have a random variable $X$ and apply a function $g(x)$, the new variable $Y = g(X)$ has its own distribution.

*   **Discrete Case**: $P(Y=y) = \sum_{x: g(x)=y} P(X=x)$.
*   **Continuous Case (Monotonic)**: If $g$ is differentiable and strictly increasing/decreasing, the PDF of $Y$ is:
    $$f_Y(y) = f_X(g^{-1}(y)) \left| \frac{d}{dy} g^{-1}(y) \right|$$
*   **Method of CDFs**: A universal approach where you first find $F_Y(y) = P(g(X) \le y)$ and then differentiate to find the density $f_Y(y)$.

---

### **`Topic 3.3` Distribution of a Random Variable**

The **Distribution** (or Law) $P_X$ is the pushforward measure defined by $P_X(B) = P(X^{-1}(B))$.

**`Def. 3.3.1` Probability Mass Function (PMF) - [Discrete]**:
$$p_X(x) = P(X = x)$$
*   **Property**: $\sum_x p_X(x) = 1$.

**`Def. 3.3.2` Probability Density Function (PDF) - [Continuous]**:
A function $f_X(x)$ such that the probability of $X$ falling in an interval is the area under the curve:
$$P(a \le X \le b) = \int_{a}^{b} f_X(x) dx$$
*   **Property**: $\int_{-\infty}^{\infty} f_X(x) dx = 1$.

**`Def. 3.3.3` Cumulative Distribution Function (CDF) - [Universal]**:
The function $F_X(x)$ calculating the probability that $X$ takes a value less than or equal to $x$:
$$F_X(x) = P(X \le x)$$
*   **Relation**: $f_X(x) = \frac{d}{dx} F_X(x)$.

---

### **`Topic 3.4` Characteristic and Moment-Generating Functions**

| Feature | Moment-Generating Function (MGF) | Characteristic Function (CF) |
| :--- | :--- | :--- |
| **Formula** | $M_X(t) = E[e^{tX}]$ | $\phi_X(t) = E[e^{itX}]$ |
| **Existence** | Not guaranteed for all distributions. | **Always exists** for any RV. |
| **Primary Use** | Generating raw moments. | Theoretical proofs (e.g., CLT). |

---

### **`Topic 3.5` Moments and Cumulants**

**`Def. 3.5.1` Moments**:
*   **n-th Raw Moment**: $\mu'_n = E[X^n]$. (Note: $\mu'_1 = \text{Mean}$).
*   **n-th Central Moment**: $\mu_n = E[(X - E[X])^n]$. (Note: $\mu_2 = \text{Variance}$).

**`Def. 3.5.2` Cumulants**:
Derived from the **Cumulant Generating Function** $K_X(t) = \ln(M_X(t))$.
*   $\kappa_1 = E[X]$ (Mean)
*   $\kappa_2 = Var(X)$ (Variance)

---

### **`Topic 3.6` Families of Distributions**

*   **Location-Scale Families**: Related by shifting ($x+\mu$) or scaling ($\sigma x$). 
*   **Exponential Families**: Distributions whose PDF/PMF can be expressed as $f(x|\theta) = h(x) \exp(\eta(\theta) \cdot T(x) - A(\theta))$. Includes Normal, Binomial, and Poisson.

---

### **`Topic 3.7` Statistics**

A **Statistic** is a measurable function of the sample data $T(X_1, \dots, X_n)$ that does not depend on unknown parameters.
*   **Example**: Sample Mean $\bar{X} = \frac{1}{n} \sum X_i$.
*   **Goal**: Statistics are used as **Estimators** to infer the value of population **Parameters**.

---

### **`Topic 3.8` Entropy ($H$)**

Entropy measures the average uncertainty or information content in a random variable.
*   **Discrete (Shannon)**: $H(X) = -\sum_{x} p(x) \log p(x)$
*   **Continuous (Differential)**: $H(X) = -\int f(x) \log f(x) dx$

---

### **`Topic 3.9` Independent Random Variables & Marginals**

**`Def. 3.9.1` Independence**:
$X$ and $Y$ are independent ($X \perp Y$) if their joint probability factors:
$$P(X \in A, Y \in B) = P(X \in A)P(Y \in B)$$
*   **Property**: $Cov(X, Y) = 0$ and $E[XY] = E[X]E[Y]$.

**`Def. 3.9.2` Marginal Distributions**:
The probability distribution of a single variable within a joint system, found by "integrating out" the other variables:
$$f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x, y) dy$$

**`Def. 3.9.3` Joint and Conditional Distributions**:
*   **Joint PDF**: $f_{X,Y}(x, y)$ (Both happening together).
*   **Conditional PDF**: $f_{X|Y}(x|y) = \frac{f_{X,Y}(x, y)}{f_Y(y)}$ (Probability of $X$ given $Y$ has occurred).
